{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro-1",
   "metadata": {},
   "source": [
    "# OpenAI Thinking Parameters with GPT-5.2\n",
    "\n",
    "This notebook demonstrates OpenAI's GPT-5.2 reasoning capabilities using the **Responses API**.\n",
    "\n",
    "## What Are Reasoning Models?\n",
    "\n",
    "Reasoning models emit **hidden reasoning tokens** before generating their final answer. This allows them to:\n",
    "- Break down complex problems into steps\n",
    "- Explore multiple approaches before committing to an answer\n",
    "- Verify their work and catch mistakes\n",
    "- Handle multi-step tasks more reliably\n",
    "\n",
    "Think of it like showing your work in math class - the model \"thinks through\" the problem before answering.\n",
    "\n",
    "## GPT-5.2 Overview\n",
    "\n",
    "| Feature | Value |\n",
    "|---------|-------|\n",
    "| Context Window | 400K tokens |\n",
    "| Input Cost | $1.75 per M tokens |\n",
    "| Cached Input | $0.175 per M tokens |\n",
    "| Output Cost | $14.00 per M tokens |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Responses API Basics](#responses-api)\n",
    "3. [Reasoning Effort Parameter](#reasoning-effort)\n",
    "4. [Verbosity Control](#verbosity)\n",
    "5. [Tool Use with Reasoning](#tools)\n",
    "6. [Structured Outputs](#structured)\n",
    "7. [Best Practices](#best-practices)\n",
    "8. [Practical Examples](#examples)\n",
    "9. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup\n",
    "\n",
    "First, let's set up our environment and initialize the OpenAI client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will:\n",
    "- Load your API key from `.env` (or be prompted once)\n",
    "- Create the `OpenAI()` client\n",
    "- Confirm setup with a short status message\n",
    "\n",
    "Expected output: a single line confirming the client is initialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized (ready for API calls)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if it exists)\n",
    "load_dotenv()\n",
    "\n",
    "# Set API key\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "print(\"OpenAI client initialized (ready for API calls)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-api-header",
   "metadata": {},
   "source": [
    "<a id='responses-api'></a>\n",
    "## 2. Responses API Basics\n",
    "\n",
    "GPT-5.2 uses the **Responses API** (`client.responses.create`) instead of the older Chat Completions API. This API is specifically designed for reasoning models.\n",
    "\n",
    "### Key Differences from Chat Completions\n",
    "\n",
    "| Feature | Chat Completions | Responses API |\n",
    "|---------|-----------------|---------------|\n",
    "| Method | `chat.completions.create()` | `responses.create()` |\n",
    "| Messages | `messages=[...]` | `input=[...]` |\n",
    "| Roles | system, user, assistant | developer, user |\n",
    "| Output | `response.choices[0].message.content` | `response.output_text` |\n",
    "| Reasoning | Not available | `reasoning={\"effort\": \"...\"}` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: see the smallest possible request/response loop with the Responses API.\n",
    "Watch for:\n",
    "- `response.output_text` (the model's answer)\n",
    "- `response.usage.total_tokens` (cost/latency proxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-basic-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses API basic example\n",
      "------------------------------------------------------------\n",
      "Answer: Paris.\n",
      "Total tokens: 35\n"
     ]
    }
   ],
   "source": [
    "# Basic usage pattern with the Responses API\n",
    "print(\"Responses API basic example\")\n",
    "print(\"-\" * 60)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a helpful assistant that provides clear, concise answers.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response.output_text}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-effort-header",
   "metadata": {},
   "source": [
    "<a id='reasoning-effort'></a>\n",
    "## 3. Reasoning Effort Parameter\n",
    "\n",
    "GPT-5.2 supports five reasoning effort levels:\n",
    "\n",
    "| Level | Use When | Speed | Cost | Quality |\n",
    "|-------|----------|-------|------|----------|\n",
    "| **none** | No reasoning needed, fastest response | Fastest | Lowest | Basic |\n",
    "| **low** | Simple tasks with minimal reasoning | Fast | Low | Good |\n",
    "| **medium** | Balanced default for most workflows | Moderate | Moderate | Great |\n",
    "| **high** | Complex multi-step tasks, critical accuracy | Slower | Higher | Best |\n",
    "| **xhigh** | Extremely complex problems, maximum accuracy | Slowest | Highest | Maximum |\n",
    "\n",
    "**Key Insight**: Higher effort = more reasoning tokens = better accuracy but higher latency/cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the same model with different `reasoning.effort` levels.\n",
    "As you go, compare output quality and the reported reasoning token counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-effort-none",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning effort = none (sentiment classification)\n",
      "------------------------------------------------------------\n",
      "Predicted sentiment: negative\n",
      "Total tokens: 44\n"
     ]
    }
   ],
   "source": [
    "# Example: No reasoning - Quick classification\n",
    "print(\"Reasoning effort = none (sentiment classification)\")\n",
    "print(\"-\" * 60)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"none\"},\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Classify the sentiment as: positive, neutral, or negative. Return only one word.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The new update completely broke my workflow. Very disappointed.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Predicted sentiment: {response.output_text}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-effort-medium",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 21) (2174513491.py, line 21)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"Generated function and tests:\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 21)\n"
     ]
    }
   ],
   "source": [
    "# Example: Medium reasoning - Code generation\n",
    "print(\"Reasoning effort = medium (email validator)\")\n",
    "print(\"-\" * 60)\n",
    "prompt = '''\n",
    "Write a Python function that validates an email address.\n",
    "Requirements:\n",
    "- Check for @ symbol\n",
    "- Verify domain has at least one dot\n",
    "- Ensure no spaces\n",
    "- Return True/False\n",
    "\n",
    "Include a docstring and 2-3 test cases.\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(\"Generated function and tests:\")\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)\n",
    "print(f\"Reasoning tokens: {getattr(response.usage.output_tokens_details, 'reasoning_tokens', 'N/A')}\")\n",
    "print(f\"Output tokens: {response.usage.output_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-effort-high",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: High reasoning - Complex algorithmic problem\n",
    "print(\"Reasoning effort = high (longest palindromic substring)\")\n",
    "print(\"-\" * 60)\n",
    "problem = '''\n",
    "Design an algorithm to find the longest palindromic substring in a string.\n",
    "\n",
    "Requirements:\n",
    "- Handle edge cases (empty string, single character, no palindromes)\n",
    "- Optimize for time complexity\n",
    "- Provide the implementation in Python\n",
    "- Explain the approach and time/space complexity\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    input=[{\"role\": \"user\", \"content\": problem}]\n",
    ")\n",
    "\n",
    "print(\"Model response:\\n\")\n",
    "print(response.output_text)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Reasoning tokens: {getattr(response.usage.output_tokens_details, 'reasoning_tokens', 'N/A')}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-effort-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison of reasoning efforts\n",
    "print(\"Reasoning effort side-by-side comparison\")\n",
    "print(\"-\" * 60)\n",
    "task = '''\n",
    "You have a list of meeting times in 24-hour format as strings: \n",
    "[\"09:00-10:30\", \"10:00-11:00\", \"14:00-15:30\", \"15:00-16:00\"]\n",
    "\n",
    "Write a Python function that finds all overlapping meetings.\n",
    "Return a list of tuples showing which meetings overlap.\n",
    "'''\n",
    "\n",
    "efforts = [\"none\", \"low\", \"medium\", \"high\"]\n",
    "results = {}\n",
    "\n",
    "for effort in efforts:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        reasoning={\"effort\": effort},\n",
    "        input=[{\"role\": \"user\", \"content\": task}]\n",
    "    )\n",
    "\n",
    "    results[effort] = {\n",
    "        \"output\": response.output_text,\n",
    "        \"reasoning_tokens\": getattr(response.usage.output_tokens_details, 'reasoning_tokens', 'N/A'),\n",
    "        \"output_tokens\": response.usage.output_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "for effort in efforts:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"REASONING EFFORT: {effort.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\n",
    "        \"Token usage: \"\n",
    "        f\"Reasoning={results[effort]['reasoning_tokens']}, \"\n",
    "        f\"Output={results[effort]['output_tokens']}, \"\n",
    "        f\"Total={results[effort]['total_tokens']}\"\n",
    "    )\n",
    "    print(f\"\\nOutput preview (first 300 chars):\\n{results[effort]['output'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-verbosity-header",
   "metadata": {},
   "source": [
    "<a id='verbosity'></a>\n",
    "## 4. Verbosity Control\n",
    "\n",
    "GPT-5.2 introduces a **verbosity** parameter to control output length. This is **independent from reasoning depth** - you can have high reasoning with concise output, or low reasoning with verbose output.\n",
    "\n",
    "| Verbosity | Description |\n",
    "|-----------|-------------|\n",
    "| **low** | Concise, bullet-point style answers |\n",
    "| **medium** | Balanced explanations (default) |\n",
    "| **high** | Detailed, comprehensive responses |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we keep reasoning effort fixed and vary verbosity to show length control.\n",
    "Notice how the answer length changes while the core meaning stays similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-verbosity-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is dependency injection and why is it useful?\"\n",
    "\n",
    "print(\"Verbosity comparison (same question, different lengths)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Try different verbosity levels\n",
    "for verbosity in [\"low\", \"medium\", \"high\"]:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        reasoning={\"effort\": \"medium\"},\n",
    "        text={\"verbosity\": verbosity},\n",
    "        input=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"VERBOSITY: {verbosity.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(response.output_text[:500] + \"...\" if len(response.output_text) > 500 else response.output_text)\n",
    "    print(f\"\\nOutput tokens: {response.usage.output_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-tools-header",
   "metadata": {},
   "source": [
    "<a id='tools'></a>\n",
    "## 5. Tool Use with Reasoning\n",
    "\n",
    "GPT-5.2 can use tools (function calling) while reasoning. The model will think about which tools to use and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples show tool calls and a web search tool.\n",
    "If the model decides a tool call is needed, you will see tool call metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tools-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools for function calling\n",
    "print(\"Tool calling example (model may request tool calls)\")\n",
    "print(\"-\" * 60)\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"City name, e.g., 'San Francisco'\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    tools=tools,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like in Tokyo and New York today?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response text:\", response.output_text if response.output_text else \"(tool calls requested)\")\n",
    "if hasattr(response, 'output') and response.output:\n",
    "    for item in response.output:\n",
    "        if hasattr(item, 'type') and item.type == 'function_call':\n",
    "            print(f\"Tool call requested: {item.name}({item.arguments})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tools-websearch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in web search tool\n",
    "print(\"Web search tool example (live data)\")\n",
    "print(\"-\" * 60)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=[{\"role\": \"user\", \"content\": \"What were the major tech announcements this week?\"}]\n",
    ")\n",
    "\n",
    "print(\"Search-based response:\\n\")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-structured-header",
   "metadata": {},
   "source": [
    "<a id='structured'></a>\n",
    "## 6. Structured Outputs\n",
    "\n",
    "Use Pydantic models to get structured, typed responses from GPT-5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Pydantic schema and ask the model to return JSON that fits it.\n",
    "You should see a parsed, typed result with labeled fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-structured-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "print(\"Structured output example (JSON schema)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "class CodeReview(BaseModel):\n",
    "    'Structured code review output'\n",
    "    issues: List[str] = Field(description=\"List of identified issues\")\n",
    "    suggestions: List[str] = Field(description=\"Improvement suggestions\")\n",
    "    severity: str = Field(description=\"Overall severity: low, medium, high\")\n",
    "    score: int = Field(description=\"Code quality score from 1-10\")\n",
    "\n",
    "code_to_review = '''\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Review code for bugs, edge cases, and improvements.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Review this code:\\n```python\\n{code_to_review}\\n```\"\n",
    "        }\n",
    "    ],\n",
    "    text={\"format\": {\"type\": \"json_schema\", \"schema\": CodeReview.model_json_schema()}}\n",
    ")\n",
    "\n",
    "import json\n",
    "review = CodeReview(**json.loads(response.output_text))\n",
    "print(f\"Score: {review.score}/10\")\n",
    "print(f\"Severity: {review.severity}\")\n",
    "print(\"\\nIssues:\")\n",
    "for issue in review.issues:\n",
    "    print(f\"  - {issue}\")\n",
    "print(\"\\nSuggestions:\")\n",
    "for suggestion in review.suggestions:\n",
    "    print(f\"  - {suggestion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-best-practices-header",
   "metadata": {},
   "source": [
    "<a id='best-practices'></a>\n",
    "## 7. Best Practices\n",
    "\n",
    "### Write Briefs, Not Prompts\n",
    "\n",
    "Reasoning models work best with comprehensive context, not chat-style prompts.\n",
    "\n",
    "### Focus on WHAT, Not HOW\n",
    "\n",
    "Let the model decide how to approach the problem. Don't micromanage the reasoning process.\n",
    "\n",
    "### Don't Ask for Chain-of-Thought\n",
    "\n",
    "Reasoning models already think internally - asking them to \"think step-by-step\" can actually degrade performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two short examples: prompt brief structure and a quick effort-level guide.\n",
    "Use these as templates for your own tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-best-practices-bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: Chat-style prompt\n",
    "print(\"Prompting best practices: bad vs good brief\")\n",
    "print(\"-\" * 60)\n",
    "bad_prompt = \"Can you help me optimize this function?\"\n",
    "\n",
    "# GOOD: Brief-style prompt with full context\n",
    "good_prompt = '''\n",
    "CONTEXT:\n",
    "I'm building a high-performance mathematics library for a financial trading system.\n",
    "The library needs to handle real-time calculations with microsecond precision.\n",
    "\n",
    "CURRENT IMPLEMENTATION:\n",
    "def calculate_fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n",
    "\n",
    "PROBLEMS:\n",
    "- Exponential time complexity O(2^n)\n",
    "- Stack overflow for n > 1000\n",
    "- Called millions of times per second in production\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Optimize for speed (target: < 1 microsecond for n < 100)\n",
    "2. Handle large values (n up to 10,000)\n",
    "3. Thread-safe implementation\n",
    "\n",
    "DELIVERABLE:\n",
    "Provide a production-ready Python implementation with time complexity analysis.\n",
    "'''\n",
    "\n",
    "print(\"BAD (lazy prompt):\")\n",
    "print(bad_prompt)\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "print(\"GOOD (comprehensive brief):\")\n",
    "print(good_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-best-practices-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the Right Effort Level\n",
    "print(\"Effort level selection guide\")\n",
    "print(\"-\" * 60)\n",
    "effort_guide = {\n",
    "    \"none\": [\"Simple classification\", \"Data extraction\", \"Formatting\"],\n",
    "    \"low\": [\"Basic Q&A\", \"Simple code fixes\", \"Summarization\"],\n",
    "    \"medium\": [\"Code generation\", \"Data analysis\", \"Most general tasks\"],\n",
    "    \"high\": [\"Algorithm design\", \"Complex debugging\", \"Architecture planning\"],\n",
    "    \"xhigh\": [\"Research problems\", \"Novel algorithm design\", \"Critical accuracy tasks\"]\n",
    "}\n",
    "\n",
    "print(\"Effort Level Selection Guide:\")\n",
    "print(\"=\" * 60)\n",
    "for effort, use_cases in effort_guide.items():\n",
    "    print(f\"\\n{effort.upper()}:\")\n",
    "    for case in use_cases:\n",
    "        print(f\"  - {case}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-examples-header",
   "metadata": {},
   "source": [
    "<a id='examples'></a>\n",
    "## 8. Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example below is a full mini-brief. Read the brief first, then run the cell\n",
    "and compare the model output to the desired deliverable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-example-code-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Code Review with Medium Reasoning\n",
    "print(\"Practical example: code review (medium effort)\")\n",
    "print(\"-\" * 60)\n",
    "buggy_code = '''\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "\n",
    "def process_user_data(data):\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        result[item['id']] = item['name'].upper()\n",
    "    return result\n",
    "'''\n",
    "\n",
    "prompt = f'''\n",
    "Review this Python code for potential bugs, edge cases, and issues.\n",
    "For each issue found:\n",
    "1. Describe the problem\n",
    "2. Show what input would cause it to fail\n",
    "3. Provide a fix\n",
    "\n",
    "Code:\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(\"Code review output:\\n\")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-example-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Business Analysis with High Reasoning\n",
    "print(\"Practical example: business analysis (high effort)\")\n",
    "print(\"-\" * 60)\n",
    "analysis_brief = '''\n",
    "COMPANY CONTEXT:\n",
    "TechStartup Inc. - B2B SaaS platform for inventory management\n",
    "- Current MRR: $125,000\n",
    "- Customer Count: 187\n",
    "- Monthly Churn Rate: 5.2%\n",
    "- Customer Acquisition Cost (CAC): $3,200\n",
    "- Customer Lifetime Value (CLV): $18,500\n",
    "\n",
    "CHALLENGE:\n",
    "High churn in SMB segment (8.5% monthly) vs Enterprise (2.1% monthly)\n",
    "\n",
    "DELIVERABLE:\n",
    "Provide 3 prioritized recommendations to reduce SMB churn, with expected impact.\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    text={\"verbosity\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": analysis_brief}]\n",
    ")\n",
    "\n",
    "print(\"Analysis output:\\n\")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-example-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Algorithm Design with High Reasoning\n",
    "print(\"Practical example: algorithm design (high effort)\")\n",
    "print(\"-\" * 60)\n",
    "algorithm_brief = '''\n",
    "PROBLEM:\n",
    "Design a rate limiter for an API gateway.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Support 10,000 requests/second per user\n",
    "- Distributed across 50 servers\n",
    "- Must handle clock skew between servers\n",
    "- Support burst traffic (allow 2x normal rate for 5 seconds)\n",
    "\n",
    "DELIVERABLE:\n",
    "1. Algorithm design with pseudocode\n",
    "2. Data structure choices with justification\n",
    "3. How to handle the distributed nature\n",
    "4. Trade-offs and limitations\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    input=[{\"role\": \"user\", \"content\": algorithm_brief}]\n",
    ")\n",
    "\n",
    "print(\"Architecture output:\\n\")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary-header",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 9. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Use the Responses API** - `client.responses.create()` is designed for reasoning models\n",
    "\n",
    "2. **Choose the right effort level**:\n",
    "   - `none`/`low`: Simple tasks, fast responses\n",
    "   - `medium`: Default for most tasks (best balance)\n",
    "   - `high`/`xhigh`: Complex problems requiring deep reasoning\n",
    "\n",
    "3. **Control verbosity separately** - Reasoning depth and output length are independent\n",
    "\n",
    "4. **Write briefs, not prompts** - Provide comprehensive context for best results\n",
    "\n",
    "5. **Don't ask for chain-of-thought** - The model reasons internally automatically\n",
    "\n",
    "6. **Use developer role** - Set system-level context with `{\"role\": \"developer\"}`\n",
    "\n",
    "### When to Use Reasoning Models\n",
    "\n",
    "**Great Use Cases:**\n",
    "- Complex code generation and debugging\n",
    "- Multi-step problem solving\n",
    "- Data analysis with multiple considerations\n",
    "- Algorithm design\n",
    "- Strategic planning\n",
    "\n",
    "**Probably Overkill:**\n",
    "- Simple text generation\n",
    "- Basic Q&A\n",
    "- Translation\n",
    "- Simple formatting tasks\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [OpenAI Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [GPT-5.2 Model Card](https://platform.openai.com/docs/models#gpt-5)\n",
    "- [Reasoning Best Practices](https://platform.openai.com/docs/guides/reasoning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}