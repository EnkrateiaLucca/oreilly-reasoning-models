{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Thinking Parameters with GPT-5.2\n",
    "\n",
    "This notebook demonstrates OpenAI's GPT-5.2 reasoning capabilities using the **Responses API**.\n",
    "\n",
    "## What Are Reasoning Models?\n",
    "\n",
    "Reasoning models emit **hidden reasoning tokens** before generating their final answer. This allows them to:\n",
    "- Break down complex problems into steps\n",
    "- Explore multiple approaches before committing to an answer\n",
    "- Verify their work and catch mistakes\n",
    "- Handle multi-step tasks more reliably\n",
    "\n",
    "Think of it like showing your work in math class - the model \"thinks through\" the problem before answering.\n",
    "\n",
    "## GPT-5.2 Overview\n",
    "\n",
    "| Feature | Value |\n",
    "|---------|-------|\n",
    "| Context Window | 400K tokens |\n",
    "| Input Cost | $1.75 per M tokens |\n",
    "| Cached Input | $0.175 per M tokens |\n",
    "| Output Cost | $14.00 per M tokens |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Responses API Basics](#responses-api)\n",
    "3. [Reasoning Effort Parameter](#reasoning-effort)\n",
    "4. [Verbosity Control](#verbosity)\n",
    "5. [Tool Use with Reasoning](#tools)\n",
    "6. [Structured Outputs](#structured)\n",
    "7. [Best Practices](#best-practices)\n",
    "8. [Practical Examples](#examples)\n",
    "9. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup\n",
    "\n",
    "First, let's set up our environment and initialize the OpenAI client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will:\n",
    "- Load your API key from `.env` (or be prompted once)\n",
    "- Create the `OpenAI()` client\n",
    "- Confirm setup with a short status message\n",
    "\n",
    "Expected output: a single line confirming the client is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized (ready for API calls)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if it exists)\n",
    "load_dotenv()\n",
    "\n",
    "# Set API key\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "print(\"OpenAI client initialized (ready for API calls)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='responses-api'></a>\n",
    "## 2. Responses API Basics\n",
    "\n",
    "GPT-5.2 uses the **Responses API** (`client.responses.create`) instead of the older Chat Completions API. This API is specifically designed for reasoning models.\n",
    "\n",
    "### Key Differences from Chat Completions\n",
    "\n",
    "| Feature | Chat Completions | Responses API |\n",
    "|---------|-----------------|---------------|\n",
    "| Method | `chat.completions.create()` | `responses.create()` |\n",
    "| Messages | `messages=[...]` | `input=[...]` |\n",
    "| Roles | system, user, assistant | developer, user |\n",
    "| Output | `response.choices[0].message.content` | `response.output_text` |\n",
    "| Reasoning | Not available | `reasoning={\"effort\": \"...\"}` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: see the smallest possible request/response loop with the Responses API.\n",
    "Watch for:\n",
    "- `response.output_text` (the model's answer)\n",
    "- `response.usage.total_tokens` (cost/latency proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses API basic example\n",
      "------------------------------------------------------------\n",
      "Answer: Paris.\n",
      "Total tokens: 35\n"
     ]
    }
   ],
   "source": [
    "# Basic usage pattern with the Responses API\n",
    "print(\"Responses API basic example\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a helpful assistant that provides clear, concise answers.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response.output_text}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reasoning-effort'></a>\n",
    "## 3. Reasoning Effort Parameter\n",
    "\n",
    "GPT-5.2 supports five reasoning effort levels:\n",
    "\n",
    "| Level | Use When | Speed | Cost | Quality |\n",
    "|-------|----------|-------|------|----------|\n",
    "| **none** | No reasoning needed, fastest response | Fastest | Lowest | Basic |\n",
    "| **low** | Simple tasks with minimal reasoning | Fast | Low | Good |\n",
    "| **medium** | Balanced default for most workflows | Moderate | Moderate | Great |\n",
    "| **high** | Complex multi-step tasks, critical accuracy | Slower | Higher | Best |\n",
    "| **xhigh** | Extremely complex problems, maximum accuracy | Slowest | Highest | Maximum |\n",
    "\n",
    "**Key Insight**: Higher effort = more reasoning tokens = better accuracy but higher latency/cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the same model with different `reasoning.effort` levels.\n",
    "As you go, compare output quality and the reported reasoning token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning effort = none (sentiment classification)\n",
      "------------------------------------------------------------\n",
      "Predicted sentiment: negative\n",
      "Total tokens: 44\n"
     ]
    }
   ],
   "source": [
    "# Example: No reasoning - Quick classification\n",
    "print(\"Reasoning effort = none (sentiment classification)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"none\"},\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Classify the sentiment as: positive, neutral, or negative. Return only one word.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The new update completely broke my workflow. Very disappointed.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Predicted sentiment: {response.output_text}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning effort = medium (email validator)\n",
      "------------------------------------------------------------\n",
      "Generated function and tests:\n",
      "```python\n",
      "def is_valid_email(email: str) -> bool:\n",
      "    \"\"\"\n",
      "    Validate a basic email address format.\n",
      "\n",
      "    Rules:\n",
      "    - Must contain exactly one '@'\n",
      "    - Must contain no spaces\n",
      "    - Domain part (after '@') must contain at least one '.'\n",
      "\n",
      "    This is a simple validator and does not fully implement RFC email rules.\n",
      "    \"\"\"\n",
      "    if not isinstance(email, str):\n",
      "        return False\n",
      "\n",
      "    if \" \" in email:\n",
      "        return False\n",
      "\n",
      "    if email.count(\"@\") != 1:\n",
      "        return False\n",
      "\n",
      "    local, domain = email.split(\"@\")\n",
      "    if not local or not domain:\n",
      "        return False\n",
      "\n",
      "    if \".\" not in domain:\n",
      "        return False\n",
      "\n",
      "    return True\n",
      "\n",
      "\n",
      "# Test cases\n",
      "print(is_valid_email(\"user@example.com\"))   # True\n",
      "print(is_valid_email(\"userexample.com\"))    # False (missing '@')\n",
      "print(is_valid_email(\"user@domain\"))        # False (domain missing '.')\n",
      "```\n",
      "\n",
      "============================================================\n",
      "Reasoning tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# Example: Medium reasoning - Code generation\n",
    "print(\"Reasoning effort = medium (email validator)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a Python function that validates an email address.\n",
    "Requirements:\n",
    "- Check for @ symbol\n",
    "- Verify domain has at least one dot\n",
    "- Ensure no spaces\n",
    "- Return True/False\n",
    "\n",
    "Include a docstring and 2-3 test cases.\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(\"Generated function and tests:\")\n",
    "print(response.output_text)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Reasoning tokens: {response.usage.output_tokens_details.reasoning_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning effort = high (longest palindromic substring)\n",
      "------------------------------------------------------------\n",
      "Model response:\n",
      "\n",
      "### Approach (Manacher’s Algorithm — **O(n)** time)\n",
      "\n",
      "A palindrome expands symmetrically around its center. A brute-force “expand around center” approach checks every center and expands, but it can take **O(n²)** in the worst case (e.g., `\"aaaaaa...\"`).\n",
      "\n",
      "**Manacher’s algorithm** optimizes this by reusing information from previously found palindromes, achieving **O(n)** time.\n",
      "\n",
      "#### Key idea\n",
      "1. **Transform the string** to handle even/odd-length palindromes uniformly by inserting separators:\n",
      "   - Original: `abba`\n",
      "   - Transformed: `^#a#b#b#a#$`\n",
      "   - `^` and `$` are sentinels to avoid bounds checks.\n",
      "2. Maintain:\n",
      "   - `C`: center of the rightmost palindrome found so far\n",
      "   - `R`: right boundary of that palindrome\n",
      "   - `P[i]`: radius of palindrome around position `i` in the transformed string\n",
      "3. For each position `i`, use its mirror around `C` to initialize `P[i]` and expand only as needed.\n",
      "\n",
      "---\n",
      "\n",
      "### Edge Cases Handling\n",
      "- **Empty string** → return `\"\"`\n",
      "- **Single character** → return the character\n",
      "- **No palindrome longer than 1** (e.g., `\"abcd\"`) → returns any one character (the algorithm naturally returns length 1)\n",
      "\n",
      "---\n",
      "\n",
      "### Python Implementation\n",
      "\n",
      "```python\n",
      "def longest_palindromic_substring(s: str) -> str:\n",
      "    \"\"\"\n",
      "    Returns the longest palindromic substring using Manacher's algorithm.\n",
      "\n",
      "    Time:  O(n)\n",
      "    Space: O(n)\n",
      "    \"\"\"\n",
      "    if not s:\n",
      "        return \"\"\n",
      "\n",
      "    # Transform: add separators to unify even/odd palindromes\n",
      "    # Example: \"abba\" -> \"^#a#b#b#a#$\"\n",
      "    t = \"^#\" + \"#\".join(s) + \"#$\"\n",
      "    n = len(t)\n",
      "\n",
      "    P = [0] * n  # P[i] = radius around i in transformed string\n",
      "    C = 0        # center of the current rightmost palindrome\n",
      "    R = 0        # right boundary of the current rightmost palindrome\n",
      "\n",
      "    for i in range(1, n - 1):\n",
      "        mirror = 2 * C - i\n",
      "\n",
      "        if i < R:\n",
      "            P[i] = min(R - i, P[mirror])\n",
      "\n",
      "        # Expand around center i\n",
      "        while t[i + 1 + P[i]] == t[i - 1 - P[i]]:\n",
      "            P[i] += 1\n",
      "\n",
      "        # Update center and right boundary if expanded past R\n",
      "        if i + P[i] > R:\n",
      "            C = i\n",
      "            R = i + P[i]\n",
      "\n",
      "    # Find the maximum length palindrome in P\n",
      "    max_len = 0\n",
      "    center_index = 0\n",
      "    for i in range(1, n - 1):\n",
      "        if P[i] > max_len:\n",
      "            max_len = P[i]\n",
      "            center_index = i\n",
      "\n",
      "    # Convert back to original string indices\n",
      "    start = (center_index - max_len) // 2\n",
      "    return s[start:start + max_len]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Complexity\n",
      "- **Time Complexity:** **O(n)**  \n",
      "  Each character is processed with amortized constant-time expansion due to reuse of palindrome radii.\n",
      "- **Space Complexity:** **O(n)**  \n",
      "  The `P` array and transformed string are linear in size.\n",
      "\n",
      "---\n",
      "\n",
      "If you want, I can also provide a simpler **expand-around-center** implementation (shorter code, **O(n²)** worst-case) for comparison.\n",
      "\n",
      "============================================================\n",
      "Reasoning tokens: 225\n",
      "Total tokens: 1050\n"
     ]
    }
   ],
   "source": [
    "# Example: High reasoning - Complex algorithmic problem\n",
    "print(\"Reasoning effort = high (longest palindromic substring)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "problem = \"\"\"\n",
    "Design an algorithm to find the longest palindromic substring in a string.\n",
    "\n",
    "Requirements:\n",
    "- Handle edge cases (empty string, single character, no palindromes)\n",
    "- Optimize for time complexity\n",
    "- Provide the implementation in Python\n",
    "- Explain the approach and time/space complexity\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    input=[{\"role\": \"user\", \"content\": problem}]\n",
    ")\n",
    "\n",
    "print(\"Model response:\\n\")\n",
    "print(response.output_text)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Reasoning tokens: {response.usage.output_tokens_details.reasoning_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning effort side-by-side comparison\n",
      "------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "REASONING EFFORT: NONE\n",
      "======================================================================\n",
      "Token usage: Reasoning=0, Output=316, Total=395\n",
      "\n",
      "Output preview (first 300 chars):\n",
      "```python\n",
      "from typing import List, Tuple\n",
      "\n",
      "def find_overlapping_meetings(meetings: List[str]) -> List[Tuple[str, str]]:\n",
      "    def to_minutes(t: str) -> int:\n",
      "        h, m = map(int, t.split(\":\"))\n",
      "        return 60 * h + m\n",
      "\n",
      "    def parse(slot: str) -> tuple[int, int]:\n",
      "        start, end = slot.split(\"-\")...\n",
      "\n",
      "======================================================================\n",
      "REASONING EFFORT: LOW\n",
      "======================================================================\n",
      "Token usage: Reasoning=0, Output=307, Total=386\n",
      "\n",
      "Output preview (first 300 chars):\n",
      "```python\n",
      "from typing import List, Tuple\n",
      "\n",
      "def find_overlapping_meetings(meetings: List[str]) -> List[Tuple[str, str]]:\n",
      "    def to_minutes(t: str) -> int:\n",
      "        h, m = map(int, t.split(\":\"))\n",
      "        return h * 60 + m\n",
      "\n",
      "    parsed = []\n",
      "    for s in meetings:\n",
      "        start_s, end_s = s.split(\"-\")\n",
      "    ...\n",
      "\n",
      "======================================================================\n",
      "REASONING EFFORT: MEDIUM\n",
      "======================================================================\n",
      "Token usage: Reasoning=120, Output=383, Total=462\n",
      "\n",
      "Output preview (first 300 chars):\n",
      "```python\n",
      "from itertools import combinations\n",
      "\n",
      "def find_overlapping_meetings(meetings):\n",
      "    def to_minutes(t):\n",
      "        h, m = map(int, t.split(\":\"))\n",
      "        return h * 60 + m\n",
      "\n",
      "    def parse(interval):\n",
      "        start_s, end_s = interval.split(\"-\")\n",
      "        return to_minutes(start_s), to_minutes(end_s)\n",
      "\n",
      "...\n",
      "\n",
      "======================================================================\n",
      "REASONING EFFORT: HIGH\n",
      "======================================================================\n",
      "Token usage: Reasoning=132, Output=472, Total=551\n",
      "\n",
      "Output preview (first 300 chars):\n",
      "```python\n",
      "from typing import List, Tuple\n",
      "\n",
      "def find_overlapping_meetings(meetings: List[str]) -> List[Tuple[str, str]]:\n",
      "    def to_minutes(hhmm: str) -> int:\n",
      "        h, m = map(int, hhmm.split(\":\"))\n",
      "        return h * 60 + m\n",
      "\n",
      "    intervals = []\n",
      "    for s in meetings:\n",
      "        start_s, end_s = s.split(...\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side comparison of reasoning efforts\n",
    "print(\"Reasoning effort side-by-side comparison\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "task = \"\"\"\n",
    "You have a list of meeting times in 24-hour format as strings: \n",
    "[\"09:00-10:30\", \"10:00-11:00\", \"14:00-15:30\", \"15:00-16:00\"]\n",
    "\n",
    "Write a Python function that finds all overlapping meetings.\n",
    "Return a list of tuples showing which meetings overlap.\n",
    "\"\"\"\n",
    "\n",
    "efforts = [\"none\", \"low\", \"medium\", \"high\"]\n",
    "results = {}\n",
    "\n",
    "for effort in efforts:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        reasoning={\"effort\": effort},\n",
    "        input=[{\"role\": \"user\", \"content\": task}]\n",
    "    )\n",
    "\n",
    "    results[effort] = {\n",
    "        \"output\": response.output_text,\n",
    "        \"reasoning_tokens\": response.usage.output_tokens_details.reasoning_tokens,\n",
    "        \"output_tokens\": response.usage.output_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "for effort in efforts:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"REASONING EFFORT: {effort.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\n",
    "        \"Token usage: \"\n",
    "        f\"Reasoning={results[effort]['reasoning_tokens']}, \"\n",
    "        f\"Output={results[effort]['output_tokens']}, \"\n",
    "        f\"Total={results[effort]['total_tokens']}\"\n",
    "    )\n",
    "    print(f\"\\nOutput preview (first 300 chars):\\n{results[effort]['output'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='verbosity'></a>\n",
    "## 4. Verbosity Control\n",
    "\n",
    "GPT-5.2 introduces a **verbosity** parameter to control output length. This is **independent from reasoning depth** - you can have high reasoning with concise output, or low reasoning with verbose output.\n",
    "\n",
    "| Verbosity | Description |\n",
    "|-----------|-------------|\n",
    "| **low** | Concise, bullet-point style answers |\n",
    "| **medium** | Balanced explanations (default) |\n",
    "| **high** | Detailed, comprehensive responses |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we keep reasoning effort fixed and vary verbosity to show length control.\n",
    "Notice how the answer length changes while the core meaning stays similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbosity comparison (same question, different lengths)\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "VERBOSITY: LOW\n",
      "============================================================\n",
      "Dependency injection (DI) is a design pattern where an object *receives* the other objects (“dependencies”) it needs from the outside, instead of creating them itself (e.g., via `new` inside the class). Those dependencies are typically provided through a constructor, a setter, or a framework/container.\n",
      "\n",
      "### What it looks like (idea)\n",
      "Without DI (class constructs its dependency):\n",
      "```java\n",
      "class OrderService {\n",
      "  private final PaymentGateway gateway = new StripeGateway(); // hard-coded\n",
      "}\n",
      "```\n",
      "\n",
      "With DI...\n",
      "\n",
      "Output tokens: 300\n",
      "\n",
      "============================================================\n",
      "VERBOSITY: MEDIUM\n",
      "============================================================\n",
      "Dependency Injection (DI) is a design pattern for building software where an object (a “dependent”) does **not create** the other objects it needs (its “dependencies”). Instead, those dependencies are **provided (“injected”) from the outside**, usually by a framework, a factory, or a caller.\n",
      "\n",
      "### What it looks like\n",
      "**Without DI (object creates its own dependency):**\n",
      "```python\n",
      "class OrderService:\n",
      "    def __init__(self):\n",
      "        self.repo = SqlOrderRepository()  # hard-coded dependency\n",
      "```\n",
      "\n",
      "**With...\n",
      "\n",
      "Output tokens: 571\n",
      "\n",
      "============================================================\n",
      "VERBOSITY: HIGH\n",
      "============================================================\n",
      "Dependency Injection (DI) is a design technique where an object *does not create* the other objects (“dependencies”) it needs to do its work. Instead, those dependencies are *provided (“injected”)* from the outside—typically via a constructor, a setter/property, or a method parameter.\n",
      "\n",
      "Put simply: **your code depends on abstractions, and something else wires the concrete implementations together.**\n",
      "\n",
      "---\n",
      "\n",
      "## 1) What problem DI solves\n",
      "\n",
      "Most non-trivial classes need collaborators:\n",
      "\n",
      "- A `UserService...\n",
      "\n",
      "Output tokens: 1235\n"
     ]
    }
   ],
   "source": [
    "question = \"What is dependency injection and why is it useful?\"\n",
    "\n",
    "print(\"Verbosity comparison (same question, different lengths)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Try different verbosity levels\n",
    "for verbosity in [\"low\", \"medium\", \"high\"]:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        reasoning={\"effort\": \"medium\"},\n",
    "        text={\"verbosity\": verbosity},\n",
    "        input=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"VERBOSITY: {verbosity.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    output_text = response.output_text\n",
    "    print(output_text[:500] + \"...\" if len(output_text) > 500 else output_text)\n",
    "    print(f\"\\nOutput tokens: {response.usage.output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tools'></a>\n",
    "## 5. Tool Use with Reasoning\n",
    "\n",
    "GPT-5.2 can use tools (function calling) while reasoning. The model will think about which tools to use and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples show tool calls and a web search tool.\n",
    "If the model decides a tool call is needed, you will see tool call metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'send_email(\"lucasinstructor@gmail.com\", \"hi\", \"lucas you are great!\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def send_email(to, subject, body):\n",
    "    print(f\"Sending email to {to} with subject {subject} and body {body}\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    instructions=\"\"\"You are a helpful email assistant with access \n",
    "    to an email tool: send_email(to, subject, body), if the user asks for an email to be sent\n",
    "    the output should be just the tool call: likeL: 'send_email(to, subject, body)'.\n",
    "    \"\"\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"Send an email to Lucas under lucasinstructor@gmail.com with subject 'hi' and body: 'lucas you are great!'\"}]\n",
    ")\n",
    "\n",
    "response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending email to lucasinstructor@gmail.com with subject hi and body lucas you are great!\n"
     ]
    }
   ],
   "source": [
    "eval(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calling example (model may request tool calls)\n",
      "------------------------------------------------------------\n",
      "Response text: (tool calls requested)\n",
      "Tool call requested: get_weather({\"location\":\"Tokyo\",\"unit\":\"celsius\"})\n",
      "Tool call requested: get_weather({\"location\":\"New York\",\"unit\":\"celsius\"})\n"
     ]
    }
   ],
   "source": [
    "# Define tools for function calling\n",
    "print(\"Tool calling example (model may request tool calls)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"City name, e.g., 'San Francisco'\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    tools=tools,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like in Tokyo and New York today?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response text:\", response.output_text if response.output_text else \"(tool calls requested)\")\n",
    "if hasattr(response, 'output') and response.output:\n",
    "    for item in response.output:\n",
    "        if hasattr(item, 'type') and item.type == 'function_call':\n",
    "            print(f\"Tool call requested: {item.name}({item.arguments})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search tool example (live data)\n",
      "------------------------------------------------------------\n",
      "Search-based response:\n",
      "\n",
      "## Major tech announcements this week (US week of **Feb 2–Feb 5, 2026**)\n",
      "\n",
      "### OpenAI: **“Frontier”** enterprise platform for AI agents\n",
      "- OpenAI announced **Frontier**, a platform for enterprises to **build, deploy, and manage AI agents** (“AI co‑workers”) that can work with files, analyze data, and execute code, with integrations into common business systems. ([axios.com](https://www.axios.com/2026/02/05/openai-platform-ai-agents?utm_source=openai))\n",
      "\n",
      "### Google Search/Discover: **February 2026 Discover “core update”**\n",
      "- Google rolled out a **broad update** to systems that rank/surface content in **Google Discover**, starting with **English-language users in the US**, with a rollout expected to take **up to two weeks**. ([searchengineland.com](https://searchengineland.com/google-releases-discover-core-update-february-2026-468308?utm_source=openai))\n",
      "\n",
      "### Nintendo: **Switch 2 game lineup revealed (Nintendo Direct)**\n",
      "- Nintendo unveiled a **Switch 2** lineup featuring major third‑party releases (e.g., *Fallout 4: Anniversary Edition*, *FF7 Rebirth*, *Resident Evil* titles) with several **dated releases** spanning Feb–June 2026 and beyond. ([theverge.com](https://www.theverge.com/games/874282/nintendo-direct-february-2026-fallout-ff7?utm_source=openai))\n",
      "\n",
      "### Microsoft/Xbox: **Game Pass February 2026 “Wave 1” lineup**\n",
      "- Microsoft announced the first **February 2026 Wave 1** additions to **Xbox Game Pass**, with titles arriving starting **Feb 3** and continuing through mid‑month (e.g., *Madden NFL 26* on **Feb 5**, *High on Life 2* on **Feb 13**). ([news.xbox.com](https://news.xbox.com/en-us/2026/02/03/xbox-game-pass-february-2026-wave-1/?utm_source=openai))\n",
      "\n",
      "### Samsung (mobile): **Galaxy F70e 5G launch date + key specs (India)**\n",
      "- Samsung announced **Galaxy F70e 5G** will debut **Feb 9, 2026**, highlighting features like a **50MP camera** and **120Hz** display. ([timesofindia.indiatimes.com](https://timesofindia.indiatimes.com/technology/mobiles-tabs/samsung-galaxy-f70e-5g-to-debut-on-february-9-50mp-camera-120hz-refresh-rate-and-more/articleshow/127862062.cms?utm_source=openai))\n",
      "\n",
      "If you tell me what you consider “major” (AI models, consumer gadgets, enterprise/cloud, gaming, startups), I can tighten this to just the top items in that category.\n"
     ]
    }
   ],
   "source": [
    "# Built-in web search tool\n",
    "print(\"Web search tool example (live data)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=[{\"role\": \"user\", \"content\": \"What were the major tech announcements this week?Output in markdown format.\"}]\n",
    ")\n",
    "\n",
    "print(\"Search-based response:\\n\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Major tech announcements this week (US week of **Feb 2–Feb 5, 2026**)\n",
       "\n",
       "### OpenAI: **“Frontier”** enterprise platform for AI agents\n",
       "- OpenAI announced **Frontier**, a platform for enterprises to **build, deploy, and manage AI agents** (“AI co‑workers”) that can work with files, analyze data, and execute code, with integrations into common business systems. ([axios.com](https://www.axios.com/2026/02/05/openai-platform-ai-agents?utm_source=openai))\n",
       "\n",
       "### Google Search/Discover: **February 2026 Discover “core update”**\n",
       "- Google rolled out a **broad update** to systems that rank/surface content in **Google Discover**, starting with **English-language users in the US**, with a rollout expected to take **up to two weeks**. ([searchengineland.com](https://searchengineland.com/google-releases-discover-core-update-february-2026-468308?utm_source=openai))\n",
       "\n",
       "### Nintendo: **Switch 2 game lineup revealed (Nintendo Direct)**\n",
       "- Nintendo unveiled a **Switch 2** lineup featuring major third‑party releases (e.g., *Fallout 4: Anniversary Edition*, *FF7 Rebirth*, *Resident Evil* titles) with several **dated releases** spanning Feb–June 2026 and beyond. ([theverge.com](https://www.theverge.com/games/874282/nintendo-direct-february-2026-fallout-ff7?utm_source=openai))\n",
       "\n",
       "### Microsoft/Xbox: **Game Pass February 2026 “Wave 1” lineup**\n",
       "- Microsoft announced the first **February 2026 Wave 1** additions to **Xbox Game Pass**, with titles arriving starting **Feb 3** and continuing through mid‑month (e.g., *Madden NFL 26* on **Feb 5**, *High on Life 2* on **Feb 13**). ([news.xbox.com](https://news.xbox.com/en-us/2026/02/03/xbox-game-pass-february-2026-wave-1/?utm_source=openai))\n",
       "\n",
       "### Samsung (mobile): **Galaxy F70e 5G launch date + key specs (India)**\n",
       "- Samsung announced **Galaxy F70e 5G** will debut **Feb 9, 2026**, highlighting features like a **50MP camera** and **120Hz** display. ([timesofindia.indiatimes.com](https://timesofindia.indiatimes.com/technology/mobiles-tabs/samsung-galaxy-f70e-5g-to-debut-on-february-9-50mp-camera-120hz-refresh-rate-and-more/articleshow/127862062.cms?utm_source=openai))\n",
       "\n",
       "If you tell me what you consider “major” (AI models, consumer gadgets, enterprise/cloud, gaming, startups), I can tighten this to just the top items in that category."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='structured'></a>\n",
    "## 6. Structured Outputs\n",
    "\n",
    "Use Pydantic models to get structured, typed responses from GPT-5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Pydantic schema and ask the model to return JSON that fits it.\n",
    "You should see a parsed, typed result with labeled fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output example (JSON schema)\n",
      "------------------------------------------------------------\n",
      "Score: 6/10\n",
      "Severity: medium\n",
      "\n",
      "Issues:\n",
      "  - Division by zero when `numbers` is empty (`len(numbers) == 0`).\n",
      "  - Assumes `numbers` supports `len()`; generators/iterators will raise `TypeError`.\n",
      "  - No input validation: non-numeric items (e.g., strings, `None`) will cause `TypeError` during addition or division.\n",
      "  - Potentially ambiguous behavior for non-standard numeric types (e.g., `Decimal`, `Fraction`) when mixing with `int`/`float`, depending on inputs.\n",
      "\n",
      "Suggestions:\n",
      "  - Handle the empty case explicitly (e.g., raise `ValueError` or return `0.0` depending on desired semantics).\n",
      "  - If you want to support any iterable (including generators), either convert to a list/tuple first or compute count manually while iterating.\n",
      "  - Use Python’s built-in `sum()` for clarity and potential performance improvements: `return sum(numbers) / len(numbers)` (still needs empty handling).\n",
      "  - Consider adding type hints and a docstring to specify expected input and behavior for edge cases.\n",
      "  - Optionally validate elements are numbers (or rely on duck typing but document it).\n",
      "  - If numerical stability matters for large lists of floats, consider `math.fsum(numbers)` instead of `sum(numbers)`.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "print(\"Structured output example (JSON schema)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "class CodeReview(BaseModel):\n",
    "    \"\"\"Structured code review output\"\"\"\n",
    "    issues: List[str] = Field(description=\"List of identified issues\")\n",
    "    suggestions: List[str] = Field(description=\"Improvement suggestions\")\n",
    "    severity: str = Field(description=\"Overall severity: low, medium, high\")\n",
    "    score: int = Field(description=\"Code quality score from 1-10\")\n",
    "\n",
    "code_to_review = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Review code for bugs, edge cases, and improvements.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Review this code:\\n```python\\n{code_to_review}\\n```\"\n",
    "        }\n",
    "    ],\n",
    "    text_format=CodeReview\n",
    ")\n",
    "\n",
    "review = response.output_parsed\n",
    "print(f\"Score: {review.score}/10\")\n",
    "print(f\"Severity: {review.severity}\")\n",
    "print(\"\\nIssues:\")\n",
    "for issue in review.issues:\n",
    "    print(f\"  - {issue}\")\n",
    "print(\"\\nSuggestions:\")\n",
    "for suggestion in review.suggestions:\n",
    "    print(f\"  - {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best-practices'></a>\n",
    "## 7. Best Practices\n",
    "\n",
    "### Write Briefs, Not Prompts\n",
    "\n",
    "Reasoning models work best with comprehensive context, not chat-style prompts.\n",
    "\n",
    "### Focus on WHAT, Not HOW\n",
    "\n",
    "Let the model decide how to approach the problem. Don't micromanage the reasoning process.\n",
    "\n",
    "### Don't Ask for Chain-of-Thought\n",
    "\n",
    "Reasoning models already think internally - asking them to \"think step-by-step\" can actually degrade performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two short examples: prompt brief structure and a quick effort-level guide.\n",
    "Use these as templates for your own tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: Chat-style prompt\n",
    "print(\"Prompting best practices: bad vs good brief\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "bad_prompt = \"Can you help me optimize this function?\"\n",
    "\n",
    "# GOOD: Brief-style prompt with full context\n",
    "good_prompt = \"\"\"\n",
    "CONTEXT:\n",
    "I'm building a high-performance mathematics library for a financial trading system.\n",
    "The library needs to handle real-time calculations with microsecond precision.\n",
    "\n",
    "CURRENT IMPLEMENTATION:\n",
    "def calculate_fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n",
    "\n",
    "PROBLEMS:\n",
    "- Exponential time complexity O(2^n)\n",
    "- Stack overflow for n > 1000\n",
    "- Called millions of times per second in production\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Optimize for speed (target: < 1 microsecond for n < 100)\n",
    "2. Handle large values (n up to 10,000)\n",
    "3. Thread-safe implementation\n",
    "\n",
    "DELIVERABLE:\n",
    "Provide a production-ready Python implementation with time complexity analysis.\n",
    "\"\"\"\n",
    "\n",
    "print(\"BAD (lazy prompt):\")\n",
    "print(bad_prompt)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\"GOOD (comprehensive brief):\")\n",
    "print(good_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the Right Effort Level\n",
    "print(\"Effort level selection guide\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "effort_guide = {\n",
    "    \"none\": [\"Simple classification\", \"Data extraction\", \"Formatting\"],\n",
    "    \"low\": [\"Basic Q&A\", \"Simple code fixes\", \"Summarization\"],\n",
    "    \"medium\": [\"Code generation\", \"Data analysis\", \"Most general tasks\"],\n",
    "    \"high\": [\"Algorithm design\", \"Complex debugging\", \"Architecture planning\"],\n",
    "    \"xhigh\": [\"Research problems\", \"Novel algorithm design\", \"Critical accuracy tasks\"]\n",
    "}\n",
    "\n",
    "print(\"Effort Level Selection Guide:\")\n",
    "print(\"=\"*60)\n",
    "for effort, use_cases in effort_guide.items():\n",
    "    print(f\"\\n{effort.upper()}:\")\n",
    "    for case in use_cases:\n",
    "        print(f\"  - {case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='examples'></a>\n",
    "## 8. Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example below is a full mini-brief. Read the brief first, then run the cell\n",
    "and compare the model output to the desired deliverable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Code Review with Medium Reasoning\n",
    "print(\"Practical example: code review (medium effort)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "buggy_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "\n",
    "def process_user_data(data):\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        result[item['id']] = item['name'].upper()\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Review this Python code for potential bugs, edge cases, and issues.\n",
    "For each issue found:\n",
    "1. Describe the problem\n",
    "2. Show what input would cause it to fail\n",
    "3. Provide a fix\n",
    "\n",
    "Code:\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(\"Code review output:\\n\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Business Analysis with High Reasoning\n",
    "print(\"Practical example: business analysis (high effort)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "analysis_brief = \"\"\"\n",
    "COMPANY CONTEXT:\n",
    "TechStartup Inc. - B2B SaaS platform for inventory management\n",
    "- Current MRR: $125,000\n",
    "- Customer Count: 187\n",
    "- Monthly Churn Rate: 5.2%\n",
    "- Customer Acquisition Cost (CAC): $3,200\n",
    "- Customer Lifetime Value (CLV): $18,500\n",
    "\n",
    "CHALLENGE:\n",
    "High churn in SMB segment (8.5% monthly) vs Enterprise (2.1% monthly)\n",
    "\n",
    "DELIVERABLE:\n",
    "Provide 3 prioritized recommendations to reduce SMB churn, with expected impact.\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    text={\"verbosity\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": analysis_brief}]\n",
    ")\n",
    "\n",
    "print(\"Analysis output:\\n\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Algorithm Design with High Reasoning\n",
    "print(\"Practical example: algorithm design (high effort)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "algorithm_brief = \"\"\"\n",
    "PROBLEM:\n",
    "Design a rate limiter for an API gateway.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Support 10,000 requests/second per user\n",
    "- Distributed across 50 servers\n",
    "- Must handle clock skew between servers\n",
    "- Support burst traffic (allow 2x normal rate for 5 seconds)\n",
    "\n",
    "DELIVERABLE:\n",
    "1. Algorithm design with pseudocode\n",
    "2. Data structure choices with justification\n",
    "3. How to handle the distributed nature\n",
    "4. Trade-offs and limitations\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    input=[{\"role\": \"user\", \"content\": algorithm_brief}]\n",
    ")\n",
    "\n",
    "print(\"Architecture output:\\n\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 9. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Use the Responses API** - `client.responses.create()` is designed for reasoning models\n",
    "\n",
    "2. **Choose the right effort level**:\n",
    "   - `none`/`low`: Simple tasks, fast responses\n",
    "   - `medium`: Default for most tasks (best balance)\n",
    "   - `high`/`xhigh`: Complex problems requiring deep reasoning\n",
    "\n",
    "3. **Control verbosity separately** - Reasoning depth and output length are independent\n",
    "\n",
    "4. **Write briefs, not prompts** - Provide comprehensive context for best results\n",
    "\n",
    "5. **Don't ask for chain-of-thought** - The model reasons internally automatically\n",
    "\n",
    "6. **Use developer role** - Set system-level context with `{\"role\": \"developer\"}`\n",
    "\n",
    "### When to Use Reasoning Models\n",
    "\n",
    "**Great Use Cases:**\n",
    "- Complex code generation and debugging\n",
    "- Multi-step problem solving\n",
    "- Data analysis with multiple considerations\n",
    "- Algorithm design\n",
    "- Strategic planning\n",
    "\n",
    "**Probably Overkill:**\n",
    "- Simple text generation\n",
    "- Basic Q&A\n",
    "- Translation\n",
    "- Simple formatting tasks\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [OpenAI Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [GPT-5.2 Model Card](https://platform.openai.com/docs/models#gpt-5)\n",
    "- [Reasoning Best Practices](https://platform.openai.com/docs/guides/reasoning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
