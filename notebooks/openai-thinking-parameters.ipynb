{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro-1",
   "metadata": {},
   "source": [
    "# OpenAI Thinking Parameters with GPT-5.2\n",
    "\n",
    "This notebook demonstrates OpenAI's GPT-5.2 reasoning capabilities using the **Responses API**.\n",
    "\n",
    "## What Are Reasoning Models?\n",
    "\n",
    "Reasoning models emit **hidden reasoning tokens** before generating their final answer. This allows them to:\n",
    "- Break down complex problems into steps\n",
    "- Explore multiple approaches before committing to an answer\n",
    "- Verify their work and catch mistakes\n",
    "- Handle multi-step tasks more reliably\n",
    "\n",
    "Think of it like showing your work in math class - the model \"thinks through\" the problem before answering.\n",
    "\n",
    "## GPT-5.2 Overview\n",
    "\n",
    "| Feature | Value |\n",
    "|---------|-------|\n",
    "| Context Window | 400K tokens |\n",
    "| Input Cost | $1.75 per M tokens |\n",
    "| Cached Input | $0.175 per M tokens |\n",
    "| Output Cost | $14.00 per M tokens |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-toc",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "2. [Responses API Basics](#responses-api)\n",
    "3. [Reasoning Effort Parameter](#reasoning-effort)\n",
    "4. [Verbosity Control](#verbosity)\n",
    "5. [Tool Use with Reasoning](#tools)\n",
    "6. [Structured Outputs](#structured)\n",
    "7. [Best Practices](#best-practices)\n",
    "8. [Practical Examples](#examples)\n",
    "9. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup\n",
    "\n",
    "First, let's set up our environment and initialize the OpenAI client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will:\n",
    "- Load your API key from `.env` (or be prompted once)\n",
    "- Create the `OpenAI()` client\n",
    "- Confirm setup with a short status message\n",
    "\n",
    "Expected output: a single line confirming the client is initialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized (ready for API calls)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if it exists)\n",
    "load_dotenv()\n",
    "\n",
    "# Set API key\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "print(\"OpenAI client initialized (ready for API calls)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-api-header",
   "metadata": {},
   "source": [
    "<a id='responses-api'></a>\n",
    "## 2. Responses API Basics\n",
    "\n",
    "GPT-5.2 uses the **Responses API** (`client.responses.create`) instead of the older Chat Completions API. This API is specifically designed for reasoning models.\n",
    "\n",
    "### Key Differences from Chat Completions\n",
    "\n",
    "| Feature | Chat Completions | Responses API |\n",
    "|---------|-----------------|---------------|\n",
    "| Method | `chat.completions.create()` | `responses.create()` |\n",
    "| Messages | `messages=[...]` | `input=[...]` |\n",
    "| Roles | system, user, assistant | developer, user |\n",
    "| Output | `response.choices[0].message.content` | `response.output_text` |\n",
    "| Reasoning | Not available | `reasoning={\"effort\": \"...\"}` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc036466",
   "metadata": {},
   "source": [
    "Goal: see the smallest possible request/response loop with the Responses API.\n",
    "Watch for:\n",
    "- `response.output_text` (the model's answer)\n",
    "- `response.usage.total_tokens` (cost/latency proxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-basic-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses API basic example\n",
      "------------------------------------------------------------\n",
      "Answer: Paris.\n",
      "Total tokens: 35\n"
     ]
    }
   ],
   "source": [
    "# Basic usage pattern with the Responses API\n",
    "print(\"Responses API basic example\")\n",
    "print(\"-\" * 60)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a helpful assistant that provides clear, concise answers.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Answer: {response.output_text}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-effort-header",
   "metadata": {},
   "source": [
    "<a id='reasoning-effort'></a>\n",
    "## 3. Reasoning Effort Parameter\n",
    "\n",
    "GPT-5.2 supports five reasoning effort levels:\n",
    "\n",
    "| Level | Use When | Speed | Cost | Quality |\n",
    "|-------|----------|-------|------|----------|\n",
    "| **none** | No reasoning needed, fastest response | Fastest | Lowest | Basic |\n",
    "| **low** | Simple tasks with minimal reasoning | Fast | Low | Good |\n",
    "| **medium** | Balanced default for most workflows | Moderate | Moderate | Great |\n",
    "| **high** | Complex multi-step tasks, critical accuracy | Slower | Higher | Best |\n",
    "| **xhigh** | Extremely complex problems, maximum accuracy | Slowest | Highest | Maximum |\n",
    "\n",
    "**Key Insight**: Higher effort = more reasoning tokens = better accuracy but higher latency/cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdd334",
   "metadata": {},
   "source": [
    "We will run the same model with different `reasoning.effort` levels.\n",
    "As you go, compare output quality and the reported reasoning token counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-effort-none",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning effort = none (sentiment classification)\n",
      "------------------------------------------------------------\n",
      "Predicted sentiment: negative\n",
      "Total tokens: 44\n"
     ]
    }
   ],
   "source": [
    "# Example: No reasoning - Quick classification\n",
    "print(\"Reasoning effort = none (sentiment classification)\")\n",
    "print(\"-\" * 60)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"none\"},\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Classify the sentiment as: positive, neutral, or negative. Return only one word.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The new update completely broke my workflow. Very disappointed.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Predicted sentiment: {response.output_text}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-effort-medium",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 21) (2174513491.py, line 21)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"Generated function and tests:\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 21)\n"
     ]
    }
   ],
   "source": [
    "# Example: Medium reasoning - Code generation\n",
    "print(\"Reasoning effort = medium (email validator)\")\n",
    "print(\"-\" * 60)\n",
    "prompt = '''\n",
    "Write a Python function that validates an email address.\n",
    "Requirements:\n",
    "- Check for @ symbol\n",
    "- Verify domain has at least one dot\n",
    "- Ensure no spaces\n",
    "- Return True/False\n",
    "\n",
    "Include a docstring and 2-3 test cases.\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(\"Generated function and tests:\")\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)\n",
    "print(f\"Reasoning tokens: {getattr(response.usage.output_tokens_details, 'reasoning_tokens', 'N/A')}\")\n",
    "print(f\"Output tokens: {response.usage.output_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-effort-high",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: High reasoning - Complex algorithmic problem\n",
    "print(\"Reasoning effort = high (longest palindromic substring)\")\n",
    "print(\"-\" * 60)\n",
    "problem = '''\n",
    "Design an algorithm to find the longest palindromic substring in a string.\n",
    "\n",
    "Requirements:\n",
    "- Handle edge cases (empty string, single character, no palindromes)\n",
    "- Optimize for time complexity\n",
    "- Provide the implementation in Python\n",
    "- Explain the approach and time/space complexity\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    input=[{\"role\": \"user\", \"content\": problem}]\n",
    ")\n",
    "\n",
    "print(\"Model response:\\n\")\n",
    "print(response.output_text)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Reasoning tokens: {getattr(response.usage.output_tokens_details, 'reasoning_tokens', 'N/A')}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-effort-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison of reasoning efforts\n",
    "print(\"Reasoning effort side-by-side comparison\")\n",
    "print(\"-\" * 60)\n",
    "task = '''\n",
    "You have a list of meeting times in 24-hour format as strings: \n",
    "[\"09:00-10:30\", \"10:00-11:00\", \"14:00-15:30\", \"15:00-16:00\"]\n",
    "\n",
    "Write a Python function that finds all overlapping meetings.\n",
    "Return a list of tuples showing which meetings overlap.\n",
    "'''\n",
    "\n",
    "efforts = [\"none\", \"low\", \"medium\", \"high\"]\n",
    "results = {}\n",
    "\n",
    "for effort in efforts:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        reasoning={\"effort\": effort},\n",
    "        input=[{\"role\": \"user\", \"content\": task}]\n",
    "    )\n",
    "\n",
    "    results[effort] = {\n",
    "        \"output\": response.output_text,\n",
    "        \"reasoning_tokens\": getattr(response.usage.output_tokens_details, 'reasoning_tokens', 'N/A'),\n",
    "        \"output_tokens\": response.usage.output_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "for effort in efforts:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"REASONING EFFORT: {effort.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\n",
    "        \"Token usage: \"\n",
    "        f\"Reasoning={results[effort]['reasoning_tokens']}, \"\n",
    "        f\"Output={results[effort]['output_tokens']}, \"\n",
    "        f\"Total={results[effort]['total_tokens']}\"\n",
    "    )\n",
    "    print(f\"\\nOutput preview (first 300 chars):\\n{results[effort]['output'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-verbosity-header",
   "metadata": {},
   "source": [
    "<a id='verbosity'></a>\n",
    "## 4. Verbosity Control\n",
    "\n",
    "GPT-5.2 introduces a **verbosity** parameter to control output length. This is **independent from reasoning depth** - you can have high reasoning with concise output, or low reasoning with verbose output.\n",
    "\n",
    "| Verbosity | Description |\n",
    "|-----------|-------------|\n",
    "| **low** | Concise, bullet-point style answers |\n",
    "| **medium** | Balanced explanations (default) |\n",
    "| **high** | Detailed, comprehensive responses |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f71a43",
   "metadata": {},
   "source": [
    "Here we keep reasoning effort fixed and vary verbosity to show length control.\n",
    "Notice how the answer length changes while the core meaning stays similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-verbosity-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is dependency injection and why is it useful?\"\n",
    "\n",
    "print(\"Verbosity comparison (same question, different lengths)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Try different verbosity levels\n",
    "for verbosity in [\"low\", \"medium\", \"high\"]:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        reasoning={\"effort\": \"medium\"},\n",
    "        text={\"verbosity\": verbosity},\n",
    "        input=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"VERBOSITY: {verbosity.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(response.output_text[:500] + \"...\" if len(response.output_text) > 500 else response.output_text)\n",
    "    print(f\"\\nOutput tokens: {response.usage.output_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-tools-header",
   "metadata": {},
   "source": [
    "<a id='tools'></a>\n",
    "## 5. Tool Use with Reasoning\n",
    "\n",
    "GPT-5.2 can use tools (function calling) while reasoning. The model will think about which tools to use and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae25cd4",
   "metadata": {},
   "source": [
    "These examples show tool calls and a web search tool.\n",
    "If the model decides a tool call is needed, you will see tool call metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tools-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools for function calling\n",
    "print(\"Tool calling example (model may request tool calls)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Responses API uses a flat structure for function tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get the current weather for a location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City name, e.g., 'San Francisco'\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    tools=tools,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like in Tokyo and New York today?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response text:\", response.output_text if response.output_text else \"(tool calls requested)\")\n",
    "if hasattr(response, 'output') and response.output:\n",
    "    for item in response.output:\n",
    "        if hasattr(item, 'type') and item.type == 'function_call':\n",
    "            print(f\"Tool call requested: {item.name}({item.arguments})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-tools-websearch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in web search tool\n",
    "print(\"Web search tool example (live data)\")\n",
    "print(\"-\" * 60)\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=[{\"role\": \"user\", \"content\": \"What were the major tech announcements this week?\"}]\n",
    ")\n",
    "\n",
    "print(\"Search-based response:\\n\")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-structured-header",
   "metadata": {},
   "source": [
    "<a id='structured'></a>\n",
    "## 6. Structured Outputs\n",
    "\n",
    "Use Pydantic models to get structured, typed responses from GPT-5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85e9aa",
   "metadata": {},
   "source": [
    "We define a Pydantic schema and ask the model to return JSON that fits it.\n",
    "You should see a parsed, typed result with labeled fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-structured-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output example (JSON schema)\n",
      "------------------------------------------------------------\n",
      "Score: 6/10\n",
      "Severity: medium\n",
      "\n",
      "Issues:\n",
      "  - Division by zero when `numbers` is empty (`len(numbers) == 0`).\n",
      "  - Assumes `numbers` supports `len()`; will fail for iterators/generators (`TypeError`).\n",
      "  - No input validation: non-numeric elements (or `None`) will raise `TypeError` during `total += num`.\n",
      "  - Potentially unclear behavior for non-list iterables (e.g., tuples work, sets work, but generators don’t).\n",
      "\n",
      "Suggestions:\n",
      "  - Handle empty input explicitly (raise a clear `ValueError`, or return `0`/`None` depending on intended semantics).\n",
      "  - If you want to support any iterable (including generators), avoid `len()` and compute count during iteration.\n",
      "  - Use built-in `sum()` for clarity and likely better performance for lists/tuples.\n",
      "  - Optionally add type hints and docstring to clarify expected input and behavior.\n",
      "  - Consider `statistics.mean(numbers)` if you want a standard-library implementation (still needs handling for empty iterables).\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "print(\"Structured output example (JSON schema)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "class CodeReview(BaseModel):\n",
    "    '''Structured code review output'''\n",
    "    issues: List[str] = Field(description=\"List of identified issues\")\n",
    "    suggestions: List[str] = Field(description=\"Improvement suggestions\")\n",
    "    severity: str = Field(description=\"Overall severity: low, medium, high\")\n",
    "    score: int = Field(description=\"Code quality score from 1-10\")\n",
    "\n",
    "code_to_review = '''\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "'''\n",
    "\n",
    "# Build a strict schema with additionalProperties: false (required by OpenAI)\n",
    "schema = CodeReview.model_json_schema()\n",
    "schema[\"additionalProperties\"] = False\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Review code for bugs, edge cases, and improvements.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Review this code:\\n```python\\n{code_to_review}\\n```\"\n",
    "        }\n",
    "    ],\n",
    "    text={\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"name\": \"code_review\",\n",
    "            \"schema\": schema,\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "import json\n",
    "review = CodeReview(**json.loads(response.output_text))\n",
    "print(f\"Score: {review.score}/10\")\n",
    "print(f\"Severity: {review.severity}\")\n",
    "print(\"\\nIssues:\")\n",
    "for issue in review.issues:\n",
    "    print(f\"  - {issue}\")\n",
    "print(\"\\nSuggestions:\")\n",
    "for suggestion in review.suggestions:\n",
    "    print(f\"  - {suggestion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-best-practices-header",
   "metadata": {},
   "source": [
    "<a id='best-practices'></a>\n",
    "## 7. Best Practices\n",
    "\n",
    "### Write Briefs, Not Prompts\n",
    "\n",
    "Reasoning models work best with comprehensive context, not chat-style prompts.\n",
    "\n",
    "### Focus on WHAT, Not HOW\n",
    "\n",
    "Let the model decide how to approach the problem. Don't micromanage the reasoning process.\n",
    "\n",
    "### Don't Ask for Chain-of-Thought\n",
    "\n",
    "Reasoning models already think internally - asking them to \"think step-by-step\" can actually degrade performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two short examples: prompt brief structure and a quick effort-level guide.\n",
    "Use these as templates for your own tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-best-practices-bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting best practices: bad vs good brief\n",
      "------------------------------------------------------------\n",
      "BAD (lazy prompt):\n",
      "Can you help me optimize this function?\n",
      "\n",
      "============================================================\n",
      "\n",
      "GOOD (comprehensive brief):\n",
      "\n",
      "CONTEXT:\n",
      "I'm building a high-performance mathematics library for a financial trading system.\n",
      "The library needs to handle real-time calculations with microsecond precision.\n",
      "\n",
      "CURRENT IMPLEMENTATION:\n",
      "def calculate_fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n",
      "\n",
      "PROBLEMS:\n",
      "- Exponential time complexity O(2^n)\n",
      "- Stack overflow for n > 1000\n",
      "- Called millions of times per second in production\n",
      "\n",
      "REQUIREMENTS:\n",
      "1. Optimize for speed (target: < 1 microsecond for n < 100)\n",
      "2. Handle large values (n up to 10,000)\n",
      "3. Thread-safe implementation\n",
      "\n",
      "DELIVERABLE:\n",
      "Provide a production-ready Python implementation with time complexity analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BAD: Chat-style prompt\n",
    "print(\"Prompting best practices: bad vs good brief\")\n",
    "print(\"-\" * 60)\n",
    "bad_prompt = \"Can you help me optimize this function?\"\n",
    "\n",
    "# GOOD: Brief-style prompt with full context\n",
    "good_prompt = '''\n",
    "CONTEXT:\n",
    "I'm building a high-performance mathematics library for a financial trading system.\n",
    "The library needs to handle real-time calculations with microsecond precision.\n",
    "\n",
    "CURRENT IMPLEMENTATION:\n",
    "def calculate_fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n",
    "\n",
    "PROBLEMS:\n",
    "- Exponential time complexity O(2^n)\n",
    "- Stack overflow for n > 1000\n",
    "- Called millions of times per second in production\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Optimize for speed (target: < 1 microsecond for n < 100)\n",
    "2. Handle large values (n up to 10,000)\n",
    "3. Thread-safe implementation\n",
    "\n",
    "DELIVERABLE:\n",
    "Provide a production-ready Python implementation with time complexity analysis.\n",
    "'''\n",
    "\n",
    "print(\"BAD (lazy prompt):\")\n",
    "print(bad_prompt)\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "print(\"GOOD (comprehensive brief):\")\n",
    "print(good_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-best-practices-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effort level selection guide\n",
      "------------------------------------------------------------\n",
      "Effort Level Selection Guide:\n",
      "============================================================\n",
      "\n",
      "NONE:\n",
      "  - Simple classification\n",
      "  - Data extraction\n",
      "  - Formatting\n",
      "\n",
      "LOW:\n",
      "  - Basic Q&A\n",
      "  - Simple code fixes\n",
      "  - Summarization\n",
      "\n",
      "MEDIUM:\n",
      "  - Code generation\n",
      "  - Data analysis\n",
      "  - Most general tasks\n",
      "\n",
      "HIGH:\n",
      "  - Algorithm design\n",
      "  - Complex debugging\n",
      "  - Architecture planning\n",
      "\n",
      "XHIGH:\n",
      "  - Research problems\n",
      "  - Novel algorithm design\n",
      "  - Critical accuracy tasks\n"
     ]
    }
   ],
   "source": [
    "# Choosing the Right Effort Level\n",
    "print(\"Effort level selection guide\")\n",
    "print(\"-\" * 60)\n",
    "effort_guide = {\n",
    "    \"none\": [\"Simple classification\", \"Data extraction\", \"Formatting\"],\n",
    "    \"low\": [\"Basic Q&A\", \"Simple code fixes\", \"Summarization\"],\n",
    "    \"medium\": [\"Code generation\", \"Data analysis\", \"Most general tasks\"],\n",
    "    \"high\": [\"Algorithm design\", \"Complex debugging\", \"Architecture planning\"],\n",
    "    \"xhigh\": [\"Research problems\", \"Novel algorithm design\", \"Critical accuracy tasks\"]\n",
    "}\n",
    "\n",
    "print(\"Effort Level Selection Guide:\")\n",
    "print(\"=\" * 60)\n",
    "for effort, use_cases in effort_guide.items():\n",
    "    print(f\"\\n{effort.upper()}:\")\n",
    "    for case in use_cases:\n",
    "        print(f\"  - {case}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-examples-header",
   "metadata": {},
   "source": [
    "<a id='examples'></a>\n",
    "## 8. Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example below is a full mini-brief. Read the brief first, then run the cell\n",
    "and compare the model output to the desired deliverable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-example-code-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practical example: code review (medium effort)\n",
      "------------------------------------------------------------\n",
      "Code review output:\n",
      "\n",
      "### 1) `calculate_average`: division by zero on empty input\n",
      "**Problem**: `len(numbers)` can be `0`, causing `ZeroDivisionError`.\n",
      "\n",
      "**Failing input**\n",
      "```python\n",
      "calculate_average([])\n",
      "```\n",
      "\n",
      "**Fix**\n",
      "```python\n",
      "def calculate_average(numbers):\n",
      "    if not numbers:\n",
      "        raise ValueError(\"numbers must be a non-empty iterable\")\n",
      "    total = 0\n",
      "    for num in numbers:\n",
      "        total += num\n",
      "    return total / len(numbers)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 2) `calculate_average`: `numbers` may be non-sized (generator), breaking `len(numbers)`\n",
      "**Problem**: If `numbers` is an iterator/generator, `len(numbers)` raises `TypeError`.\n",
      "\n",
      "**Failing input**\n",
      "```python\n",
      "calculate_average(x for x in [1, 2, 3])\n",
      "```\n",
      "\n",
      "**Fix** (consume once and count)\n",
      "```python\n",
      "def calculate_average(numbers):\n",
      "    total = 0\n",
      "    count = 0\n",
      "    for num in numbers:\n",
      "        total += num\n",
      "        count += 1\n",
      "    if count == 0:\n",
      "        raise ValueError(\"numbers must be a non-empty iterable\")\n",
      "    return total / count\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 3) `calculate_average`: non-numeric elements cause runtime errors\n",
      "**Problem**: `total += num` fails if `num` isn’t numeric (or mixes incompatible types).\n",
      "\n",
      "**Failing inputs**\n",
      "```python\n",
      "calculate_average([1, \"2\", 3])      # TypeError\n",
      "calculate_average([1, None, 3])     # TypeError\n",
      "```\n",
      "\n",
      "**Fix** (validate or coerce; here we validate)\n",
      "```python\n",
      "def calculate_average(numbers):\n",
      "    total = 0.0\n",
      "    count = 0\n",
      "    for num in numbers:\n",
      "        if not isinstance(num, (int, float)):\n",
      "            raise TypeError(f\"All elements must be numeric; got {type(num).__name__}\")\n",
      "        total += num\n",
      "        count += 1\n",
      "    if count == 0:\n",
      "        raise ValueError(\"numbers must be a non-empty iterable\")\n",
      "    return total / count\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 4) `process_user_data`: `data` may be `None` or non-iterable\n",
      "**Problem**: `for item in data:` fails if `data` is `None` or not iterable.\n",
      "\n",
      "**Failing inputs**\n",
      "```python\n",
      "process_user_data(None)   # TypeError: 'NoneType' is not iterable\n",
      "process_user_data(123)    # TypeError: 'int' is not iterable\n",
      "```\n",
      "\n",
      "**Fix**\n",
      "```python\n",
      "def process_user_data(data):\n",
      "    if data is None:\n",
      "        raise ValueError(\"data must be an iterable of user records\")\n",
      "    result = {}\n",
      "    for item in data:\n",
      "        # handled below\n",
      "        ...\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 5) `process_user_data`: items may not be dicts / missing required keys\n",
      "**Problem**: `item['id']` / `item['name']` raises errors if `item` isn’t a dict-like object or keys are missing.\n",
      "\n",
      "**Failing inputs**\n",
      "```python\n",
      "process_user_data([(\"id\", 1)])                 # TypeError (tuple indexing by str)\n",
      "process_user_data([{\"id\": 1}])                 # KeyError: 'name'\n",
      "process_user_data([{\"name\": \"Alice\"}])         # KeyError: 'id'\n",
      "```\n",
      "\n",
      "**Fix** (explicit validation)\n",
      "```python\n",
      "def process_user_data(data):\n",
      "    if data is None:\n",
      "        raise ValueError(\"data must be an iterable of user records\")\n",
      "    result = {}\n",
      "    for item in data:\n",
      "        if not isinstance(item, dict):\n",
      "            raise TypeError(f\"Each item must be a dict; got {type(item).__name__}\")\n",
      "        if \"id\" not in item or \"name\" not in item:\n",
      "            raise KeyError(\"Each item must contain 'id' and 'name'\")\n",
      "        result[item[\"id\"]] = str(item[\"name\"]).upper()\n",
      "    return result\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 6) `process_user_data`: `name` might not be a string (no `.upper()`)\n",
      "**Problem**: If `name` is not a string (e.g., `None`, int), `.upper()` fails.\n",
      "\n",
      "**Failing inputs**\n",
      "```python\n",
      "process_user_data([{\"id\": 1, \"name\": None}])   # AttributeError\n",
      "process_user_data([{\"id\": 2, \"name\": 123}])    # AttributeError\n",
      "```\n",
      "\n",
      "**Fix** (convert to string or reject; example converts)\n",
      "```python\n",
      "# inside loop\n",
      "name = item[\"name\"]\n",
      "result[item[\"id\"]] = str(name).upper()\n",
      "```\n",
      "\n",
      "(Or, if you prefer strictness: `if not isinstance(name, str): raise TypeError(...)`)\n",
      "\n",
      "---\n",
      "\n",
      "### 7) `process_user_data`: `id` might be unhashable (can’t be a dict key)\n",
      "**Problem**: Dictionary keys must be hashable; if `id` is a list/dict, assignment fails.\n",
      "\n",
      "**Failing input**\n",
      "```python\n",
      "process_user_data([{\"id\": [], \"name\": \"Alice\"}])  # TypeError: unhashable type: 'list'\n",
      "```\n",
      "\n",
      "**Fix** (validate hashability)\n",
      "```python\n",
      "def process_user_data(data):\n",
      "    if data is None:\n",
      "        raise ValueError(\"data must be an iterable of user records\")\n",
      "    result = {}\n",
      "    for item in data:\n",
      "        if not isinstance(item, dict):\n",
      "            raise TypeError(f\"Each item must be a dict; got {type(item).__name__}\")\n",
      "        if \"id\" not in item or \"name\" not in item:\n",
      "            raise KeyError(\"Each item must contain 'id' and 'name'\")\n",
      "\n",
      "        user_id = item[\"id\"]\n",
      "        try:\n",
      "            hash(user_id)\n",
      "        except TypeError:\n",
      "            raise TypeError(f\"'id' must be hashable; got {type(user_id).__name__}\")\n",
      "\n",
      "        result[user_id] = str(item[\"name\"]).upper()\n",
      "    return result\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 8) `process_user_data`: duplicate IDs overwrite silently\n",
      "**Problem**: If two items share the same `'id'`, the later one overwrites the earlier without warning.\n",
      "\n",
      "**Failing input (logical failure / data loss)**\n",
      "```python\n",
      "process_user_data([\n",
      "    {\"id\": 1, \"name\": \"Alice\"},\n",
      "    {\"id\": 1, \"name\": \"Bob\"},\n",
      "])\n",
      "# returns {1: \"BOB\"} and silently drops \"ALICE\"\n",
      "```\n",
      "\n",
      "**Fix** (choose a policy: raise, keep first, or accumulate; example raises)\n",
      "```python\n",
      "def process_user_data(data):\n",
      "    if data is None:\n",
      "        raise ValueError(\"data must be an iterable of user records\")\n",
      "    result = {}\n",
      "    for item in data:\n",
      "        if not isinstance(item, dict):\n",
      "            raise TypeError(f\"Each item must be a dict; got {type(item).__name__}\")\n",
      "        if \"id\" not in item or \"name\" not in item:\n",
      "            raise KeyError(\"Each item must contain 'id' and 'name'\")\n",
      "\n",
      "        user_id = item[\"id\"]\n",
      "        if user_id in result:\n",
      "            raise ValueError(f\"Duplicate id encountered: {user_id}\")\n",
      "\n",
      "        result[user_id] = str(item[\"name\"]).upper()\n",
      "    return result\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Consolidated “safer” version (optional)\n",
      "```python\n",
      "def calculate_average(numbers):\n",
      "    total = 0.0\n",
      "    count = 0\n",
      "    for num in numbers:\n",
      "        if not isinstance(num, (int, float)):\n",
      "            raise TypeError(f\"All elements must be numeric; got {type(num).__name__}\")\n",
      "        total += num\n",
      "        count += 1\n",
      "    if count == 0:\n",
      "        raise ValueError(\"numbers must be a non-empty iterable\")\n",
      "    return total / count\n",
      "\n",
      "\n",
      "def process_user_data(data):\n",
      "    if data is None:\n",
      "        raise ValueError(\"data must be an iterable of user records\")\n",
      "\n",
      "    result = {}\n",
      "    for item in data:\n",
      "        if not isinstance(item, dict):\n",
      "            raise TypeError(f\"Each item must be a dict; got {type(item).__name__}\")\n",
      "        if \"id\" not in item or \"name\" not in item:\n",
      "            raise KeyError(\"Each item must contain 'id' and 'name'\")\n",
      "\n",
      "        user_id = item[\"id\"]\n",
      "        try:\n",
      "            hash(user_id)\n",
      "        except TypeError:\n",
      "            raise TypeError(f\"'id' must be hashable; got {type(user_id).__name__}\")\n",
      "\n",
      "        if user_id in result:\n",
      "            raise ValueError(f\"Duplicate id encountered: {user_id}\")\n",
      "\n",
      "        result[user_id] = str(item[\"name\"]).upper()\n",
      "\n",
      "    return result\n",
      "```\n",
      "\n",
      "If you tell me what behavior you want for empty averages and duplicate IDs (raise vs return `None` vs default value vs accumulate), I can tailor the fixes to match your intended semantics.\n"
     ]
    }
   ],
   "source": [
    "# Example: Code Review with Medium Reasoning\n",
    "print(\"Practical example: code review (medium effort)\")\n",
    "print(\"-\" * 60)\n",
    "buggy_code = '''\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total / len(numbers)\n",
    "\n",
    "def process_user_data(data):\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        result[item['id']] = item['name'].upper()\n",
    "    return result\n",
    "'''\n",
    "\n",
    "prompt = f'''\n",
    "Review this Python code for potential bugs, edge cases, and issues.\n",
    "For each issue found:\n",
    "1. Describe the problem\n",
    "2. Show what input would cause it to fail\n",
    "3. Provide a fix\n",
    "\n",
    "Code:\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(\"Code review output:\\n\")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-example-business",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practical example: business analysis (high effort)\n",
      "------------------------------------------------------------\n",
      "Analysis output:\n",
      "\n",
      "SMB customers are driving most of the churn. Using your overall churn (5.2%) and segment churn rates (SMB 8.5%, Ent 2.1%), SMB is ~48% of the customer base but ~80% of monthly logo churn (≈7.7 of ~9.7 customers lost/month). So focusing on SMB retention will move the company-level number meaningfully.\n",
      "\n",
      "## 1) Fix SMB “time-to-value” (first 7–30 days) with a guided onboarding + templates + key integrations (Highest priority)\n",
      "**Why this matters:** SMB churn is most often caused by “never fully implemented / never got a win,” not competitive displacement. If activation is weak, no amount of later success outreach saves the account.\n",
      "\n",
      "**What to do (next 30–60 days):**\n",
      "- Define 2–3 **activation milestones** that predict retention (e.g., inventory imported, first reconciliation completed, first reorder alert configured, 2+ users active).\n",
      "- Add an **in-app onboarding checklist** + guided setup wizard (CSV import mapping, SKU normalization, location setup).\n",
      "- Provide **industry templates** (common SMB verticals: retail, light manufacturing, 3PL-lite) with default workflows/reports.\n",
      "- Prioritize the top SMB **integrations** that remove manual work (e.g., QuickBooks/Xero, Shopify/WooCommerce, ShipStation, common barcode scanners).\n",
      "\n",
      "**Expected impact:**\n",
      "- **Reduce SMB churn by ~1.5–2.5 percentage points** (8.5% → ~6.0–7.0%) within ~2–3 months.\n",
      "- Company-wide churn improvement: roughly **0.7–1.2 pts** (because SMB is ~48% of customers).\n",
      "\n",
      "---\n",
      "\n",
      "## 2) Launch “scaled SMB Customer Success” with health scoring + proactive retention playbooks (Next priority)\n",
      "**Why this matters:** SMB needs automation + targeted human touches. Enterprise can justify high-touch CS; SMB retention improves when you intervene *before* usage drops and when users get periodic help adopting additional workflows.\n",
      "\n",
      "**What to do (next 60–120 days):**\n",
      "- Implement an SMB **health score** from product signals (e.g., weekly active users, new POs created, inventory adjustments, days since last login, integration disconnected).\n",
      "- Create **trigger-based plays**:\n",
      "  - “No activation by day 14” → outreach + 15-min rescue call.\n",
      "  - “Usage drop 30% WoW” → in-app nudge + office hours invite.\n",
      "  - “Integration failure / sync off” → immediate support + how-to.\n",
      "- Run **monthly SMB webinars** + weekly “implementation office hours.”\n",
      "- Add a **save/cancel flow** that routes “confused / not implemented” to enablement, not immediate cancellation.\n",
      "\n",
      "**Expected impact:**\n",
      "- **Reduce SMB churn by ~1.0–1.8 percentage points** within ~3–6 months (stacking with onboarding improvements).\n",
      "\n",
      "---\n",
      "\n",
      "## 3) Remove SMB churn from pricing friction + involuntary churn (dunning) + right-sized packaging (Third priority)\n",
      "**Why this matters:** SMB churn often includes (a) price/value mismatch as they scale up/down, and (b) involuntary churn (failed cards) that is “free” to recover.\n",
      "\n",
      "**What to do (next 30–90 days):**\n",
      "- Add **dunning** (card retries, smart retry timing, in-app/email prompts, “update payment” UX).\n",
      "- Introduce **annual/quarterly options** with modest incentives (and/or a “pause” plan) to reduce month-to-month cancellation.\n",
      "- Simplify SMB packaging around a value metric SMB understands (e.g., locations, SKUs, orders) to avoid surprise overages; offer an easy **downgrade path** instead of churn.\n",
      "\n",
      "**Expected impact:**\n",
      "- **Reduce SMB churn by ~0.5–1.0 percentage points** (often quickly, especially if involuntary churn is non-trivial).\n",
      "- Additional benefit: improved cash flow from annual prepay (even if logo churn is unchanged, realized churn decreases).\n",
      "\n",
      "---\n",
      "\n",
      "# Combined expected impact (if executed well)\n",
      "- **SMB churn:** 8.5% → **~4.7% to 6.0%** (down **2.5–3.8 points**) over ~3–6 months.\n",
      "- **Overall churn:** 5.2% → **~3.4% to 4.0%** (down **1.2–1.8 points**), assuming enterprise churn stays ~2.1%.\n",
      "- **Customer lifetime / CLV effect (directional):** Since CLV scales ~1/churn, moving SMB churn from 8.5% to 5.5% increases average SMB lifetime by ~55% (11.8 months → 18.2 months), materially improving the CLV:CAC ratio in SMB.\n",
      "\n",
      "If you share (1) SMB ARPA vs enterprise ARPA and (2) what % of churn happens in the first 90 days, I can tighten the expected $MRR impact and prioritize the exact onboarding milestones/playbooks.\n"
     ]
    }
   ],
   "source": [
    "# Example: Business Analysis with High Reasoning\n",
    "print(\"Practical example: business analysis (high effort)\")\n",
    "print(\"-\" * 60)\n",
    "analysis_brief = '''\n",
    "COMPANY CONTEXT:\n",
    "TechStartup Inc. - B2B SaaS platform for inventory management\n",
    "- Current MRR: $125,000\n",
    "- Customer Count: 187\n",
    "- Monthly Churn Rate: 5.2%\n",
    "- Customer Acquisition Cost (CAC): $3,200\n",
    "- Customer Lifetime Value (CLV): $18,500\n",
    "\n",
    "CHALLENGE:\n",
    "High churn in SMB segment (8.5% monthly) vs Enterprise (2.1% monthly)\n",
    "\n",
    "DELIVERABLE:\n",
    "Provide 3 prioritized recommendations to reduce SMB churn, with expected impact.\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    text={\"verbosity\": \"medium\"},\n",
    "    input=[{\"role\": \"user\", \"content\": analysis_brief}]\n",
    ")\n",
    "\n",
    "print(\"Analysis output:\\n\")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-example-algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Practical example: algorithm design (high effort)\n",
      "------------------------------------------------------------\n",
      "Architecture output:\n",
      "\n",
      "## 1) Algorithm design (with pseudocode)\n",
      "\n",
      "### Approach: **Two-level Token Bucket** (atomic per request)\n",
      "We enforce *both*:\n",
      "- **Sustained rate**: 10,000 req/s (long-term)\n",
      "- **Burst policy**: up to **20,000 req/s for 5 seconds**\n",
      "\n",
      "A single token bucket sized for 5s burst can allow overly-large instantaneous spikes. So we use **two buckets**:\n",
      "\n",
      "1. **Long-term bucket (sustained)**\n",
      "   - refill rate `R = 10_000 tokens/sec`\n",
      "   - capacity `C_long = (burst_multiplier-1) * R * burst_duration = (2-1)*10_000*5 = 50_000`\n",
      "   - Meaning: you can run **20k/s for 5s** (net drain 10k/s → 50k tokens consumed).\n",
      "\n",
      "2. **Short-term bucket (cap instantaneous-ish rate)**\n",
      "   - refill rate `R_short = 20_000 tokens/sec`\n",
      "   - capacity `C_short = 20_000` (≈ allow up to 20k in a ~1s window)\n",
      "\n",
      "**Decision rule:** allow request only if **both** buckets have ≥ 1 token.\n",
      "\n",
      "### State per user (stored centrally)\n",
      "For each bucket we store:\n",
      "- `tokens` (integer, fixed-point)\n",
      "- `last_refill_time_us` (int64 microseconds)\n",
      "\n",
      "### Pseudocode (single atomic operation)\n",
      "\n",
      "```pseudo\n",
      "# Constants\n",
      "R_LONG      = 10_000          # tokens/sec\n",
      "C_LONG      = 50_000          # tokens\n",
      "R_SHORT     = 20_000          # tokens/sec\n",
      "C_SHORT     = 20_000          # tokens\n",
      "SCALE_US    = 1_000_000       # convert seconds<->microseconds\n",
      "TTL_SECONDS = 3600            # expire idle users\n",
      "\n",
      "function refill(tokens, last_us, now_us, rate_per_sec, capacity):\n",
      "    delta_us = max(0, now_us - last_us)       # protect against time anomalies\n",
      "    add = floor(rate_per_sec * delta_us / SCALE_US)\n",
      "    tokens = min(capacity, tokens + add)\n",
      "    return (tokens, now_us)\n",
      "\n",
      "function allow_request(user_id, cost=1):\n",
      "    now_us = CENTRAL_TIME_US()                # do not use local server clock\n",
      "\n",
      "    # Load (tokens, last_us) for both buckets; if missing, initialize full\n",
      "    (tL, lastL, tS, lastS) = LOAD_USER_STATE(user_id)\n",
      "    if missing:\n",
      "        tL = C_LONG;  lastL = now_us\n",
      "        tS = C_SHORT; lastS = now_us\n",
      "\n",
      "    (tL, lastL) = refill(tL, lastL, now_us, R_LONG,  C_LONG)\n",
      "    (tS, lastS) = refill(tS, lastS, now_us, R_SHORT, C_SHORT)\n",
      "\n",
      "    if tL >= cost AND tS >= cost:\n",
      "        tL = tL - cost\n",
      "        tS = tS - cost\n",
      "        allowed = true\n",
      "    else:\n",
      "        allowed = false\n",
      "\n",
      "    STORE_USER_STATE(user_id, tL, lastL, tS, lastS, ttl=TTL_SECONDS)\n",
      "    return allowed\n",
      "```\n",
      "\n",
      "### Implementation note (important in distributed gateways)\n",
      "Make `allow_request()` **atomic** using:\n",
      "- Redis Lua script, or\n",
      "- a dedicated rate-limiter service with per-key serialization, or\n",
      "- a strongly-consistent KV with compare-and-swap.\n",
      "\n",
      "For Redis, use `TIME` inside Lua to get a consistent timestamp.\n",
      "\n",
      "---\n",
      "\n",
      "## 2) Data structure choices (with justification)\n",
      "\n",
      "### Central store entry per user\n",
      "**Redis Hash** (or a compact binary blob) keyed by `rl:{user_id}` with fields:\n",
      "- `tL` (int) long tokens\n",
      "- `tsL` (int64) long last refill time (us)\n",
      "- `tS` (int) short tokens\n",
      "- `tsS` (int64) short last refill time (us)\n",
      "\n",
      "**Why this structure**\n",
      "- **O(1)** read/modify/write per request\n",
      "- Constant memory per user\n",
      "- Easy TTL eviction for inactive users\n",
      "- Atomic update with a Lua script (no race conditions across 50 servers)\n",
      "- Integers avoid float drift; microseconds give good precision at 10k–20k rps\n",
      "\n",
      "**Memory**\n",
      "- Roughly tens of bytes per active user + Redis overhead. TTL prevents unbounded growth.\n",
      "\n",
      "---\n",
      "\n",
      "## 3) Handling the distributed nature (50 servers + clock skew)\n",
      "\n",
      "### A) Single logical limiter via sharded central store (recommended)\n",
      "- All 50 gateway servers call the limiter operation for each request.\n",
      "- Use **Redis Cluster** (or multiple limiter shards) so keys are distributed:\n",
      "  - Keyed by user id → each user’s state lives on exactly one shard (no cross-shard transactions needed).\n",
      "- Redis Lua script ensures atomicity per request on that shard.\n",
      "\n",
      "**Clock skew handling**\n",
      "- Do **not** trust gateway server clocks.\n",
      "- Use **Redis server time** (`TIME` in Lua) or a limiter-service monotonic clock.\n",
      "- Since refill uses `now_us` from a single authoritative source per shard, skew between gateways is irrelevant.\n",
      "\n",
      "### B) Availability and scaling considerations\n",
      "- **Replication**: Redis primary + replica per shard.\n",
      "- **Failover**: accept brief limiter unavailability trade-off:\n",
      "  - **Fail-closed** (reject when limiter down) = safest for protection\n",
      "  - **Fail-open** (allow when down) = best for availability; risks overload\n",
      "  - Common compromise: fail-open for a short window with alarms + reduced per-node local cap.\n",
      "\n",
      "### C) Optional optimization: token leasing (reduces central QPS)\n",
      "If central calls become too expensive:\n",
      "- Gateways periodically “lease” blocks of tokens per user (e.g., 500–2000 tokens) from Redis atomically, then serve requests locally until depleted.\n",
      "- Pros: far fewer Redis ops under heavy traffic\n",
      "- Cons: complexity, possible unfairness/over-allocation during bursts, harder to reason about precise limits\n",
      "\n",
      "---\n",
      "\n",
      "## 4) Trade-offs and limitations\n",
      "\n",
      "### Pros\n",
      "- **Exact global enforcement** across 50 servers (with atomic central update)\n",
      "- **Handles burst requirement** explicitly (20k/s up to ~5s, then back to 10k/s)\n",
      "- **Clock-skew safe** (central/shard time source)\n",
      "- **Low per-user state**, TTL-based cleanup\n",
      "\n",
      "### Cons / limitations\n",
      "- **Central dependency & latency**: every request adds a network hop unless token leasing is used.\n",
      "- **Hot-key contention**: a single user at 10k–20k rps hammers one Redis shard/key; Redis can handle this, but many hot users require enough shards.\n",
      "- **Token bucket semantics**: the short bucket approximates “20k per rolling 1s”; if you need stricter “no more than X in any exact sliding window,” consider GCRA or sliding-window log (more expensive).\n",
      "- **Failover edge cases**: during Redis failover you may see brief limiter pauses or inconsistent behavior depending on client retry strategy.\n",
      "\n",
      "---\n",
      "\n",
      "If you want, I can provide a Redis Lua script implementing the atomic operation (with `TIME`, fixed-point math, TTL, and return values like remaining tokens + retry-after).\n"
     ]
    }
   ],
   "source": [
    "# Example: Algorithm Design with High Reasoning\n",
    "print(\"Practical example: algorithm design (high effort)\")\n",
    "print(\"-\" * 60)\n",
    "algorithm_brief = '''\n",
    "PROBLEM:\n",
    "Design a rate limiter for an API gateway.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Support 10,000 requests/second per user\n",
    "- Distributed across 50 servers\n",
    "- Must handle clock skew between servers\n",
    "- Support burst traffic (allow 2x normal rate for 5 seconds)\n",
    "\n",
    "DELIVERABLE:\n",
    "1. Algorithm design with pseudocode\n",
    "2. Data structure choices with justification\n",
    "3. How to handle the distributed nature\n",
    "4. Trade-offs and limitations\n",
    "'''\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5.2\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    input=[{\"role\": \"user\", \"content\": algorithm_brief}]\n",
    ")\n",
    "\n",
    "print(\"Architecture output:\\n\")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary-header",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 9. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Use the Responses API** - `client.responses.create()` is designed for reasoning models\n",
    "\n",
    "2. **Choose the right effort level**:\n",
    "   - `none`/`low`: Simple tasks, fast responses\n",
    "   - `medium`: Default for most tasks (best balance)\n",
    "   - `high`/`xhigh`: Complex problems requiring deep reasoning\n",
    "\n",
    "3. **Control verbosity separately** - Reasoning depth and output length are independent\n",
    "\n",
    "4. **Write briefs, not prompts** - Provide comprehensive context for best results\n",
    "\n",
    "5. **Don't ask for chain-of-thought** - The model reasons internally automatically\n",
    "\n",
    "6. **Use developer role** - Set system-level context with `{\"role\": \"developer\"}`\n",
    "\n",
    "### When to Use Reasoning Models\n",
    "\n",
    "**Great Use Cases:**\n",
    "- Complex code generation and debugging\n",
    "- Multi-step problem solving\n",
    "- Data analysis with multiple considerations\n",
    "- Algorithm design\n",
    "- Strategic planning\n",
    "\n",
    "**Probably Overkill:**\n",
    "- Simple text generation\n",
    "- Basic Q&A\n",
    "- Translation\n",
    "- Simple formatting tasks\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [OpenAI Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n",
    "- [GPT-5.2 Model Card](https://platform.openai.com/docs/models#gpt-5)\n",
    "- [Reasoning Best Practices](https://platform.openai.com/docs/guides/reasoning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
