{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7318c13",
   "metadata": {},
   "source": [
    "# Live Session\n",
    "\n",
    "## Student question:\n",
    "\n",
    "> Can you suggest analytical frameworks, beyond artificial analysts to assess how an AI project’s token price correlates with its actual performance, usage, and fundamentals? In short it worth the investment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be242ea",
   "metadata": {},
   "source": [
    "### Task/Eval Score/Cost/Model Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ae074",
   "metadata": {},
   "source": [
    "We will build a simple evaluation pipeline:\n",
    "1) Summarize a paper\n",
    "2) Score the summary with a judge model\n",
    "3) Normalize scores into a single metric\n",
    "4) Compare models and estimate token cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1433b8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if it exists)\n",
    "load_dotenv()\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "print(\"OpenAI API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb1efa",
   "metadata": {},
   "source": [
    "## Task Definition\n",
    "\n",
    "Summarizing papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a4fa4",
   "metadata": {},
   "source": [
    "We'll define the task in code so later cells can reuse it reliably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e70e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task definition set: Evaluate how well a model can summarize a paper using an LLM-as-a-judge.\n"
     ]
    }
   ],
   "source": [
    "task_definition = (\n",
    "    \"Evaluate how well a model can summarize a paper using an LLM-as-a-judge.\"\n",
    ")\n",
    "print(f\"Task definition set: {task_definition}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b41d5d",
   "metadata": {},
   "source": [
    "How do we evaluate a summary of a paper? \n",
    "\n",
    "More importantly is, how do we quantify a decent score for this evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f073275",
   "metadata": {},
   "source": [
    "The hack will be instead of using traditional metrics, we'll use LLM-AS-A-JUDGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78facfe",
   "metadata": {},
   "source": [
    "Next we define the judge prompt and load the local PDF so we can compare summaries\n",
    "against the original paper content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23b5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_EVAL_SUMMARY = f'''\n",
    "    You are an expert evaluator. Your task is to assess the quality of the provided summary of a document using the following four criteria. For each, please think step by step before giving a score.\n",
    "\n",
    "        Evaluation Criteria:\n",
    "\n",
    "        1. Coherence (1-5) - the collective quality of all sentences in the summary. The summary should be well-structured and well-organized, building a coherent body of information about the topic.\n",
    "\n",
    "        2. Consistency (1-5) - factual alignment between the summary and the source document. The summary should contain only statements that are logically entailed by the source document.\n",
    "\n",
    "        3. Fluency (1-5) - the quality of individual sentences. The summary should be free of grammatical errors, awkward phrasing, or formatting issues that impede readability.\n",
    "\n",
    "        4. Relevance (1-5) - inclusion of the most important content from the source. The summary should focus on key information and avoid including irrelevant or redundant details.\n",
    "\n",
    "        Instructions:\n",
    "        - First, read both the source document and all the context of the task and the summary carefully.\n",
    "        - For each criterion:\n",
    "        1. Reflect step by step (e.g., \"Step-by-step reasoning: ...\").\n",
    "        2. Then provide a score from 1 (poor) to 5 (excellent) for each criterion.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "def eval_score(summary_output, context, model_judge_openai=None, model_judge_anthropic=None):\n",
    "    # Use defaults set elsewhere if not provided.\n",
    "    if model_judge_openai is None:\n",
    "        model_judge_openai = MODEL_JUDGE_OPENAI\n",
    "    if model_judge_anthropic is None:\n",
    "        model_judge_anthropic = MODEL_JUDGE_ANTHROPIC\n",
    "\n",
    "    # Use Claude when evaluating Claude outputs; otherwise use OpenAI.\n",
    "    if model_judge_anthropic and model_judge_anthropic.startswith(\"claude-\"):\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=model_judge_anthropic,\n",
    "            max_tokens=1000,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{PROMPT_EVAL_SUMMARY}\\n\\nSummary: {summary_output}\\nContext: {context}\"\n",
    "            }]\n",
    "        )\n",
    "        return _extract_anthropic_text(response.content)\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model_judge_openai,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": PROMPT_EVAL_SUMMARY},\n",
    "            {\"role\": \"user\", \"content\": f\"Summary: {summary_output}\\nContext: {context}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4a985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using paper at: /Users/greatmaster/Desktop/projects/oreilly-live-trainings/oreilly-reasoning-models/notebooks/paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# Use the repo copy of paper.pdf; avoid network downloads\n",
    "from pathlib import Path\n",
    "paper_path = Path(\"paper.pdf\")\n",
    "if not paper_path.exists():\n",
    "    raise FileNotFoundError(\"Expected paper.pdf in notebooks/; add it to the repo.\")\n",
    "print(f\"Using paper at: {paper_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bbd8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting paper.pdf to markdown with docling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greatmaster/miniconda3/envs/oreilly-reasoning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/greatmaster/miniconda3/envs/oreilly-reasoning/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/greatmaster/miniconda3/envs/oreilly-reasoning/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/greatmaster/miniconda3/envs/oreilly-reasoning/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper converted successfully.\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "print(\"Converting paper.pdf to markdown with docling...\")\n",
    "\n",
    "def convert_pdf_to_markdown(pdf_path):\n",
    "    'Converts a local PDF to markdown using docling and returns the result.'\n",
    "    try:\n",
    "        converter = DocumentConverter()\n",
    "        result = converter.convert(pdf_path)\n",
    "        markdown = result.document.export_to_markdown()\n",
    "        return markdown\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting document: {e}\")\n",
    "        return None\n",
    "\n",
    "paper_path = \"./paper.pdf\"\n",
    "\n",
    "paper_contents = convert_pdf_to_markdown(paper_path)\n",
    "if not paper_contents:\n",
    "    raise ValueError(\"Failed to convert paper.pdf to markdown\")\n",
    "print(\"Paper converted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13509a0",
   "metadata": {},
   "source": [
    "We now display the paper contents so you can skim the source text before\n",
    "summarizing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b3bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying paper contents (markdown):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<!-- image -->\n",
       "\n",
       "## Chain-of-Thought Reasoning without Prompting\n",
       "\n",
       "Xuezhi Wang 1 and Denny Zhou 1\n",
       "\n",
       "1 Google DeepMind, 1 {xuezhiw, dennyzhou}@google.com\n",
       "\n",
       "In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason e"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "print(\"Displaying paper contents (markdown):\")\n",
    "Markdown(paper_contents[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26f84997",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_JUDGE_OPENAI = \"gpt-4o\"\n",
    "MODEL_JUDGE_ANTHROPIC = \"claude-sonnet-4-5-20250929\"\n",
    "\n",
    "# Choose a default judge model for this notebook\n",
    "MODEL_JUDGE = MODEL_JUDGE_OPENAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d166bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_SUMMARIZE = \"\"\"\n",
    "You are an expert in summarizing papers. \n",
    "You take in full markdown paper content and return a comprehensive summary of it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d133b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients initialized (OpenAI + Anthropic)\n",
      "Summary generated (gpt-4o).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_JUDGE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m summary, tokens = summarize_paper(paper_contents, \u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSummary generated (gpt-4o).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m output_score = eval_score(summary, paper_contents, \u001b[43mMODEL_JUDGE\u001b[49m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSummary evaluated by judge model.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'MODEL_JUDGE' is not defined"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "openai_client = OpenAI()\n",
    "# Assumes ANTHROPIC_API_KEY is set in the environment\n",
    "anthropic_client = anthropic.Anthropic()\n",
    "\n",
    "print(\"Clients initialized (OpenAI + Anthropic)\")\n",
    "\n",
    "\n",
    "def _extract_anthropic_text(content_blocks):\n",
    "    text = \"\"\n",
    "    for block in content_blocks:\n",
    "        if getattr(block, \"type\", None) == \"text\":\n",
    "            text += block.text\n",
    "    return text\n",
    "\n",
    "\n",
    "def summarize_paper(paper_contents, model):\n",
    "    if model.startswith(\"claude-\"):\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=2000,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"{PROMPT_SUMMARIZE}\\n\\nSummarize this paper: {paper_contents}\"}\n",
    "            ]\n",
    "        )\n",
    "        text = _extract_anthropic_text(response.content)\n",
    "        tokens = (response.usage.input_tokens or 0) + (response.usage.output_tokens or 0)\n",
    "        return text, tokens\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": PROMPT_SUMMARIZE},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize this paper: {paper_contents}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content, response.usage.total_tokens\n",
    "\n",
    "summary, tokens = summarize_paper(paper_contents, \"gpt-4o\")\n",
    "print(\"Summary generated (gpt-4o).\")\n",
    "output_score = eval_score(summary, paper_contents, MODEL_JUDGE)\n",
    "print(\"Summary evaluated by judge model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5bb39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY (Markdown) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The paper \"Chain-of-Thought Reasoning without Prompting\" explores the intrinsic reasoning abilities of large language models (LLMs) without relying on the traditional prompting methods that require manual tinkering and task-specific human inputs. The authors, Xuezhi Wang and Denny Zhou from Google DeepMind, propose a novel approach called CoT-decoding to elicit reasoning paths from LLMs. This approach involves altering the decoding process by inspecting the top k alternative tokens rather than relying solely on greedy decoding. Their findings suggest that large language models inherently possess reasoning capabilities that can be brought to light by considering these alternative decoding paths.\n",
       "\n",
       "Key contributions and findings include:\n",
       "1. **Intrinsic Reasoning without Human Prompts**: The study demonstrates that decoding changes can uncover reasoning capabilities in LLMs without the need for external prompting. This challenges the assumption that LLMs lack effective reasoning without specific prompts and highlights that the perceived inability arises from neglecting non-greedy paths in decoding.\n",
       "\n",
       "2. **Correlation between CoT Paths and Model Confidence**: The presence of a chain-of-thought reasoning path is correlated with higher confidence in the final answer, suggesting that LLMs are more certain of their solutions when following a reasoning path.\n",
       "\n",
       "3. **CoT-Decoding Methodology**: The authors introduce CoT-decoding, a method that uses answer confidence to select reliable reasoning paths during the decoding process, leading to improved reasoning performance across various benchmarks without additional training data or model tuning.\n",
       "\n",
       "4. **Applicability across Models and Scales**: The approach effectively enhances reasoning in multiple LLM families, such as PaLM-2, Mistral, and Gemma, and is applicable across different model scales, providing accuracy gains on both math and commonsense reasoning tasks.\n",
       "\n",
       "5. **Task Difficulty and Pre-training Influence**: The study notes that the discovery of correct CoT paths is influenced by task difficulty and pre-training distribution, with simpler tasks revealing inherent reasoning paths more readily.\n",
       "\n",
       "6. **Combining CoT-Decoding and Prompting**: While CoT-decoding alone demonstrates significant improvements, its combination with traditional CoT-prompting methods yields even larger reasoning gains, showcasing the versatility and effectiveness of the approach.\n",
       "\n",
       "The paper concludes that LLMs have untapped reasoning potential that can be harnessed through clever decoding strategies rather than extensive prompt engineering, paving the way for further advancements in understanding and leveraging the intrinsic capabilities of these models. Additionally, they propose areas for future exploration, including further fine-tuning using discovered CoT paths and branching beyond the first decoding token to optimize reasoning paths.\n",
       "\n",
       "Overall, this research offers a transformative perspective on enhancing LLM reasoning by simplifying the decoding process and exploiting existing model capabilities rather than relying on extensive external inputs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== SUMMARY (Markdown) ===\")\n",
    "Markdown(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a175102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION OUTPUT (Markdown) ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Evaluation of Summary Quality\n",
       "\n",
       "## 1. Coherence (1-5)\n",
       "\n",
       "**Step-by-step reasoning:**\n",
       "- The summary follows a clear logical structure, starting with an introduction to the paper's main contribution\n",
       "- It systematically covers the key findings numbered 1-6, creating a coherent progression\n",
       "- Each section builds upon the previous one, from intrinsic reasoning abilities to methodology to applications\n",
       "- The conclusion naturally synthesizes the main points\n",
       "- Transitions between sections are smooth and logical\n",
       "- The organization mirrors the paper's structure while maintaining readability\n",
       "\n",
       "**Score: 5/5**\n",
       "\n",
       "The summary is exceptionally well-structured with clear organization and logical flow throughout.\n",
       "\n",
       "## 2. Consistency (1-5)\n",
       "\n",
       "**Step-by-step reasoning:**\n",
       "- Checking key claims against source document:\n",
       "  - \"Chain-of-Thought Reasoning without Prompting\" - ✓ Accurate title and authors\n",
       "  - \"Altering the decoding process by inspecting top k alternative tokens\" - ✓ Matches source\n",
       "  - \"CoT paths correlated with higher confidence\" - ✓ Supported by source (Figure 1, Table 1)\n",
       "  - \"Works across PaLM-2, Mistral, and Gemma\" - ✓ Confirmed in experiments\n",
       "  - \"Task difficulty and pre-training influence\" - ✓ Discussed in Section 3.2\n",
       "  - \"Combining with prompting yields larger gains\" - ✓ Shown in Table 7\n",
       "- No fabricated information detected\n",
       "- All numerical claims and model names match the source\n",
       "- The summary accurately represents the paper's methodology and findings\n",
       "\n",
       "**Score: 5/5**\n",
       "\n",
       "The summary is completely consistent with the source document, with no factual errors or misrepresentations.\n",
       "\n",
       "## 3. Fluency (1-5)\n",
       "\n",
       "**Step-by-step reasoning:**\n",
       "- Grammar: All sentences are grammatically correct\n",
       "- Sentence structure: Varied and sophisticated without being convoluted\n",
       "- Technical terminology: Used appropriately and consistently (e.g., \"LLMs,\" \"CoT-decoding,\" \"greedy decoding\")\n",
       "- Readability: Clear and accessible despite technical subject matter\n",
       "- No awkward phrasing detected\n",
       "- Punctuation and formatting are correct throughout\n",
       "- Professional academic tone maintained consistently\n",
       "\n",
       "**Score: 5/5**\n",
       "\n",
       "The summary is exceptionally well-written with no grammatical errors or fluency issues.\n",
       "\n",
       "## 4. Relevance (1-5)\n",
       "\n",
       "**Step-by-step reasoning:**\n",
       "- Captures the main contribution: Novel CoT-decoding approach - ✓\n",
       "- Includes key methodology: Top k alternative tokens, confidence-based selection - ✓\n",
       "- Reports important empirical results: Performance across models and tasks - ✓\n",
       "- Mentions critical insights: Intrinsic reasoning capabilities, correlation with confidence - ✓\n",
       "- Covers practical applications: Combining with prompting, applicability across scales - ✓\n",
       "- Discusses limitations appropriately: Task difficulty, pre-training influence - ✓\n",
       "- Avoids excessive technical details while maintaining substance\n",
       "- No significant irrelevant information included\n",
       "- Could have mentioned computational costs briefly (minor omission from Discussion section)\n",
       "\n",
       "**Score: 5/5**\n",
       "\n",
       "The summary captures all essential content from the paper, focusing on the most important contributions and findings without including irrelevant details.\n",
       "\n",
       "## Final Scores:\n",
       "- **Coherence: 5/5**\n",
       "- **Consistency: 5/5**\n",
       "- **Fluency: 5/5**\n",
       "- **Relevance: 5/5**\n",
       "\n",
       "This is an exemplary summary that accurately, clearly, and comprehensively represents the source document's key contributions and findings."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== EVALUATION OUTPUT (Markdown) ===\")\n",
    "Markdown(output_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc59aa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     11\u001b[39m     response = openai_client.beta.chat.completions.parse(\n\u001b[32m     12\u001b[39m         model=MODEL_JUDGE,\n\u001b[32m     13\u001b[39m         messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         response_format=SummaryOutputScore\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.parsed\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m summary_output_score = extract_summary_output_score(\u001b[43moutput_score\u001b[49m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCoherence score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_output_score.coherence_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConsistency score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_output_score.consistency_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'output_score' is not defined"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SummaryOutputScore(BaseModel):\n",
    "    coherence_score: float = Field(description=\"The score of the coherence of the summary\")\n",
    "    consistency_score: float = Field(description=\"The score of the consistency of the summary\")\n",
    "    fluency_score: float = Field(description=\"The score of the fluency of the summary\")\n",
    "    relevance_score: float = Field(description=\"The score of the relevance of the summary\")\n",
    "\n",
    "\n",
    "def extract_summary_output_score(eval_output):\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=MODEL_JUDGE,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You extract these scores from the evaluation scores output: coherence_score, consistency_score, fluency_score, relevance_score.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Extract the scores from this evaluation scores output: {eval_output}\"}\n",
    "        ],\n",
    "        response_format=SummaryOutputScore\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "summary_output_score = extract_summary_output_score(output_score)\n",
    "\n",
    "print(f\"Coherence score: {summary_output_score.coherence_score}\")\n",
    "print(f\"Consistency score: {summary_output_score.consistency_score}\")\n",
    "print(f\"Fluency score: {summary_output_score.fluency_score}\")\n",
    "print(f\"Relevance score: {summary_output_score.relevance_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9af0b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_score(scores_structured: SummaryOutputScore) -> float:\n",
    "    'Calculate the final score as the average of the four criteria (1 to 5).'\n",
    "    return (scores_structured.coherence_score + scores_structured.consistency_score +         scores_structured.fluency_score + scores_structured.relevance_score) / 4\n",
    "\n",
    "\n",
    "# final_score = calculate_final_score(summary_output_score)\n",
    "# print(f\"Final average score: {final_score:.2f}/5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48576a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example cost for 1k input/1k output (gpt-4o): $0.012500\n"
     ]
    }
   ],
   "source": [
    "token_cost_dict = {\n",
    "    \"gpt-5.2\": {\n",
    "        \"input\": 1.75 / 1_000_000,\n",
    "        \"cached_input\": 0.175 / 1_000_000,\n",
    "        \"output\": 14.00 / 1_000_000,\n",
    "    },\n",
    "    \"gpt-4o\": {\n",
    "        \"input\": 2.50 / 1_000_000,\n",
    "        \"cached_input\": 1.25 / 1_000_000,\n",
    "        \"output\": 10.00 / 1_000_000,\n",
    "    },\n",
    "    \"claude-sonnet-4-5\": {\n",
    "        \"input\": 3.00 / 1_000_000,\n",
    "        \"cached_input\": 0.30 / 1_000_000,\n",
    "        \"output\": 15.00 / 1_000_000,\n",
    "    },\n",
    "    \"gpt-5-mini\": {\n",
    "        \"input\": 0.15 / 1_000_000,\n",
    "        \"cached_input\": 0.015 / 1_000_000,\n",
    "        \"output\": 1.20 / 1_000_000,\n",
    "    },\n",
    "    \"gpt-4.1-mini\": {\n",
    "        \"input\": 0.03 / 1_000_000,\n",
    "        \"cached_input\": 0.003 / 1_000_000,\n",
    "        \"output\": 0.24 / 1_000_000,\n",
    "    },\n",
    "    \"gpt-4.1\": {\n",
    "        \"input\": 0.15 / 1_000_000,\n",
    "        \"cached_input\": 0.015 / 1_000_000,\n",
    "        \"output\": 1.20 / 1_000_000,\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_token_cost(model: str, input_tokens: int, output_tokens: int, cached_tokens: int = 0) -> float:\n",
    "    'Calculate the total cost for a model run.'\n",
    "    if model not in token_cost_dict:\n",
    "        raise ValueError(f\"Model {model} not found in token_cost_dict\")\n",
    "    prices = token_cost_dict[model]\n",
    "\n",
    "    input_cost = (input_tokens * (prices[\"input\"] or 0)) if prices[\"input\"] else 0\n",
    "    cached_cost = (cached_tokens * (prices[\"cached_input\"] or 0)) if prices[\"cached_input\"] else 0\n",
    "    output_cost = (output_tokens * (prices[\"output\"] or 0)) if prices[\"output\"] else 0\n",
    "\n",
    "    return input_cost + cached_cost + output_cost\n",
    "\n",
    "\n",
    "example_cost = get_token_cost(\"gpt-4o\", 1000, 1000, 0)\n",
    "print(f\"Example cost for 1k input/1k output (gpt-4o): ${example_cost:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fbf4596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample run token count: 32\n"
     ]
    }
   ],
   "source": [
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": 'tell jokes'},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a funny joke\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "token_count = response.usage.total_tokens\n",
    "print(f\"Sample run token count: {token_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0977d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3 summaries per model...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "num_generations = 3\n",
    "results = []\n",
    "model_options = ['gpt-5-mini', 'gpt-5.2', 'gpt-4.1']\n",
    "\n",
    "print(f\"Running {num_generations} summaries per model...\")\n",
    "\n",
    "for model in model_options:\n",
    "    for n in range(num_generations):\n",
    "        summary, tokens = summarize_paper(paper_contents, model)\n",
    "        output_score = eval_score(summary, paper_contents, model)\n",
    "        token_cost = get_token_cost(model, tokens, tokens, 0)\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"generation\": n + 1,\n",
    "            \"summary\": summary,\n",
    "            \"output_score\": output_score,\n",
    "            \"tokens\": tokens,\n",
    "            \"token_cost\": token_cost\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21220fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>generation</th>\n",
       "      <th>summary</th>\n",
       "      <th>output_score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>Summary — \"Chain-of-Thought Reasoning without ...</td>\n",
       "      <td>## Evaluation of Summary Quality\\n\\nI will eva...</td>\n",
       "      <td>22223</td>\n",
       "      <td>0.030001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-mini</td>\n",
       "      <td>2</td>\n",
       "      <td>Here’s a concise, comprehensive summary of the...</td>\n",
       "      <td># SUMMARY EVALUATION\\n\\nI'll evaluate this sum...</td>\n",
       "      <td>22020</td>\n",
       "      <td>0.029727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-5-mini</td>\n",
       "      <td>3</td>\n",
       "      <td>Paper title: Chain-of-Thought Reasoning withou...</td>\n",
       "      <td>## Evaluation of Summary Quality\\n\\nI'll evalu...</td>\n",
       "      <td>22744</td>\n",
       "      <td>0.030704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-5.2</td>\n",
       "      <td>1</td>\n",
       "      <td>## High-level idea\\n\\nThe paper asks whether *...</td>\n",
       "      <td>Looking at this summary against the source doc...</td>\n",
       "      <td>21954</td>\n",
       "      <td>0.345776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-5.2</td>\n",
       "      <td>2</td>\n",
       "      <td>## High-level idea\\n\\nThe paper argues that la...</td>\n",
       "      <td># Step-by-Step Evaluation of the Summary\\n\\n##...</td>\n",
       "      <td>21682</td>\n",
       "      <td>0.341492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-5.2</td>\n",
       "      <td>3</td>\n",
       "      <td>## Paper in one sentence\\nThe paper shows that...</td>\n",
       "      <td>I'll evaluate this summary using the four crit...</td>\n",
       "      <td>21655</td>\n",
       "      <td>0.341066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Certainly! Here is a comprehensive summary of ...</td>\n",
       "      <td>I'll evaluate this summary across the four cri...</td>\n",
       "      <td>21615</td>\n",
       "      <td>0.029180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>2</td>\n",
       "      <td>Here's a comprehensive summary of the paper **...</td>\n",
       "      <td># Evaluation of Summary\\n\\nLet me evaluate thi...</td>\n",
       "      <td>21899</td>\n",
       "      <td>0.029564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>3</td>\n",
       "      <td>Here is a comprehensive summary of the paper *...</td>\n",
       "      <td># Evaluation of the Summary\\n\\nLet me evaluate...</td>\n",
       "      <td>21655</td>\n",
       "      <td>0.029234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  generation                                            summary  \\\n",
       "0  gpt-5-mini           1  Summary — \"Chain-of-Thought Reasoning without ...   \n",
       "1  gpt-5-mini           2  Here’s a concise, comprehensive summary of the...   \n",
       "2  gpt-5-mini           3  Paper title: Chain-of-Thought Reasoning withou...   \n",
       "3     gpt-5.2           1  ## High-level idea\\n\\nThe paper asks whether *...   \n",
       "4     gpt-5.2           2  ## High-level idea\\n\\nThe paper argues that la...   \n",
       "5     gpt-5.2           3  ## Paper in one sentence\\nThe paper shows that...   \n",
       "6     gpt-4.1           1  Certainly! Here is a comprehensive summary of ...   \n",
       "7     gpt-4.1           2  Here's a comprehensive summary of the paper **...   \n",
       "8     gpt-4.1           3  Here is a comprehensive summary of the paper *...   \n",
       "\n",
       "                                        output_score  tokens  token_cost  \n",
       "0  ## Evaluation of Summary Quality\\n\\nI will eva...   22223    0.030001  \n",
       "1  # SUMMARY EVALUATION\\n\\nI'll evaluate this sum...   22020    0.029727  \n",
       "2  ## Evaluation of Summary Quality\\n\\nI'll evalu...   22744    0.030704  \n",
       "3  Looking at this summary against the source doc...   21954    0.345776  \n",
       "4  # Step-by-Step Evaluation of the Summary\\n\\n##...   21682    0.341492  \n",
       "5  I'll evaluate this summary using the four crit...   21655    0.341066  \n",
       "6  I'll evaluate this summary across the four cri...   21615    0.029180  \n",
       "7  # Evaluation of Summary\\n\\nLet me evaluate thi...   21899    0.029564  \n",
       "8  # Evaluation of the Summary\\n\\nLet me evaluate...   21655    0.029234  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dde972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95afe40e",
   "metadata": {},
   "source": [
    "This table includes the raw summaries and judge outputs for each run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecc810",
   "metadata": {},
   "source": [
    "Now we extract numeric scores and build a compact comparison table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dfe69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract summaries and structured output_scores, and create a new DataFrame\n",
    "\n",
    "# Assuming the structured output function is called `extract_structured_output`\n",
    "# and it takes the output_score (which may be a string or object) and returns a dict\n",
    "\n",
    "# Extract summaries\n",
    "summaries = [result['summary'] for result in results]\n",
    "\n",
    "# # Extract structured output scores\n",
    "structured_scores = [calculate_final_score(extract_summary_output_score(result['output_score'])) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85646747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>generation</th>\n",
       "      <th>summary</th>\n",
       "      <th>final_score</th>\n",
       "      <th>token_cost</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-mini</td>\n",
       "      <td>1</td>\n",
       "      <td>Summary — \"Chain-of-Thought Reasoning without ...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.030001</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-mini</td>\n",
       "      <td>2</td>\n",
       "      <td>Here’s a concise, comprehensive summary of the...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.029727</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-5-mini</td>\n",
       "      <td>3</td>\n",
       "      <td>Paper title: Chain-of-Thought Reasoning withou...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-5.2</td>\n",
       "      <td>1</td>\n",
       "      <td>## High-level idea\\n\\nThe paper asks whether *...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.345776</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-5.2</td>\n",
       "      <td>2</td>\n",
       "      <td>## High-level idea\\n\\nThe paper argues that la...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.341492</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-5.2</td>\n",
       "      <td>3</td>\n",
       "      <td>## Paper in one sentence\\nThe paper shows that...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.341066</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Certainly! Here is a comprehensive summary of ...</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.029180</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>2</td>\n",
       "      <td>Here's a comprehensive summary of the paper **...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.029564</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>3</td>\n",
       "      <td>Here is a comprehensive summary of the paper *...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.029234</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  generation                                            summary  \\\n",
       "0  gpt-5-mini           1  Summary — \"Chain-of-Thought Reasoning without ...   \n",
       "1  gpt-5-mini           2  Here’s a concise, comprehensive summary of the...   \n",
       "2  gpt-5-mini           3  Paper title: Chain-of-Thought Reasoning withou...   \n",
       "3     gpt-5.2           1  ## High-level idea\\n\\nThe paper asks whether *...   \n",
       "4     gpt-5.2           2  ## High-level idea\\n\\nThe paper argues that la...   \n",
       "5     gpt-5.2           3  ## Paper in one sentence\\nThe paper shows that...   \n",
       "6     gpt-4.1           1  Certainly! Here is a comprehensive summary of ...   \n",
       "7     gpt-4.1           2  Here's a comprehensive summary of the paper **...   \n",
       "8     gpt-4.1           3  Here is a comprehensive summary of the paper *...   \n",
       "\n",
       "   final_score  token_cost     0  \n",
       "0         5.00    0.030001  5.00  \n",
       "1         5.00    0.029727  5.00  \n",
       "2         5.00    0.030704  5.00  \n",
       "3         5.00    0.345776  5.00  \n",
       "4         5.00    0.341492  5.00  \n",
       "5         5.00    0.341066  5.00  \n",
       "6         4.75    0.029180  4.75  \n",
       "7         5.00    0.029564  5.00  \n",
       "8         5.00    0.029234  5.00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine into a new DataFrame\n",
    "summary_table = pd.DataFrame(structured_scores)\n",
    "summary_table['model'] = [result['model'] for result in results]\n",
    "summary_table['generation'] = [result['generation'] for result in results]\n",
    "summary_table['summary'] = summaries\n",
    "summary_table['final_score'] = structured_scores\n",
    "summary_table['token_cost'] = [result['token_cost'] for result in results]\n",
    "# Reorder columns for clarity if desired\n",
    "cols = ['model', 'generation', 'summary', 'final_score', 'token_cost'] + [col for col in summary_table.columns if col not in ['model', 'generation', 'summary', 'final_score', 'token_cost']]\n",
    "summary_table = summary_table[cols]\n",
    "\n",
    "summary_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "O'Reilly Reasoning Models",
   "language": "python",
   "name": "oreilly-reasoning"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
