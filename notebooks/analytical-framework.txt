import os
import getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"var: ")

_set_env("OPENAI_API_KEY")

model_options = ['gpt-4o', 'gpt-4o-mini', 'gpt-5', 'gpt-5-mini']

costs = []

PROMPT_EVAL_SUMMARY = f"""
    You are an expert evaluator. Your task is to assess the quality of the provided summary of a document using the following four criteria. For each, please think step by step before giving a score.

        Evaluation Criteria:

        1. Coherence (1–5) – the collective quality of all sentences in the summary. The summary should be well-structured and well-organized, building a coherent body of information about the topic.

        2. Consistency (1–5) – factual alignment between the summary and the source document. The summary should contain only statements that are logically entailed by the source document.

        3. Fluency (1–5) – the quality of individual sentences. The summary should be free of grammatical errors, awkward phrasing, or formatting issues that impede readability.

        4. Relevance (1–5) – inclusion of the most important content from the source. The summary should focus on key information and avoid including irrelevant or redundant details.

        Instructions:
        - First, **read both the source document and all the context of the task and the summary carefully**.
        - For each criterion:
        1. Reflect step by step (e.g., "Step-by-step reasoning: …").
        2. Then provide a score from **1** (poor) to **5** (excellent) for each criterion.

    """

!wget -O paper.pdf "https://arxiv.org/pdf/2402.10200"

from docling.document_converter import DocumentConverter

def convert_pdf_to_markdown(pdf_path):
    """Converts a local PDF to markdown using docling and prints the result."""
    try:
        converter = DocumentConverter()
        result = converter.convert(pdf_path)
        markdown = result.document.export_to_markdown()
        print(markdown)
        return markdown
    except Exception as e:
        print(f"Error converting document: {e}")
        return None

paper_path = "./paper.pdf"

paper_contents = convert_pdf_to_markdown(paper_path)

from IPython.display import Markdown

Markdown(paper_contents)

from openai import OpenAI

client = OpenAI()

task_definition = """
The task is to summarize a paper compressing as much as possible 
while preserving the most important information from the paper."""


MODEL_JUDGE = "o3-mini"


def eval_score(summary_output, context, model_judge=MODEL_JUDGE):
    """
    Evaluate the quality of a summary using a LLM-as-a-judge.
    
    Args:
        summary_output (str): The summary to evaluate
        context (str): The context of the paper
        model_judge (str): The model to use for the evaluation
        
    Returns:
        float: The score of the summary
    """
    
    response = client.chat.completions.create(
        model=model_judge,
        messages=[
            {"role": "system", "content": PROMPT_EVAL_SUMMARY},
            {"role": "user", "content": f"Summary: {summary_output}\nContext: {context}"}
        ]
    )
    
    return response.choices[0].message.content



context = f"""
The task is:
    {task_definition}

The paper content is:
    {paper_contents}
"""

PROMPT_SUMMARIZE = """
You are an expert in summarizing papers. 
You take in full markdown paper content and return a comprehensive summary of it.
"""

def summarize_paper(paper_contents, model):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": PROMPT_SUMMARIZE},
            {"role": "user", "content": f"Summarize this paper: {paper_contents}"}
        ]
    )
    
    return response.choices[0].message.content


summary = summarize_paper(paper_contents, "gpt-4o-mini")
output_score = eval_score(summary, paper_contents, MODEL_JUDGE)

Markdown(summary)

Markdown(output_score)

from pydantic import BaseModel, Field

class SummaryOutputScore(BaseModel):
    coherence_score: float = Field(description="The score of the coherence of the summary")
    consistency_score: float = Field(description="The score of the consistency of the summary")
    fluency_score: float = Field(description="The score of the fluency of the summary")
    relevance_score: float = Field(description="The score of the relevance of the summary")


def extract_summary_output_score(eval_output):
    response = client.beta.chat.completions.parse(
        model=MODEL_JUDGE,
        messages=[
            {"role": "system", "content": "You extract these scores from the evaluation scores output: coherence_score, consistency_score,\
                fluency_score, relevance_score."},
            {"role": "user", "content": f"Extract the scores from this evaluation scores output: {eval_output}"}
        ],
        response_format=SummaryOutputScore
    )
    
    return response.choices[0].message.parsed


summary_output_score = extract_summary_output_score(output_score)

print(summary_output_score.coherence_score)
print(summary_output_score.consistency_score)
print(summary_output_score.fluency_score)
print(summary_output_score.relevance_score)

def calculate_final_score(scores_structured: SummaryOutputScore) -> float:
    """
    Calculate the final score based on the scores of the summary.
    The final score is the average of the scores of the summary.
    The scores are between 1 and 5.
    """
    return (scores_structured.coherence_score + scores_structured.consistency_score + \
        scores_structured.fluency_score + scores_structured.relevance_score) / 4


final_score = calculate_final_score(summary_output_score)
print(final_score)

model_options = ['gpt-4o', 'gpt-4o-mini', 'gpt-5', 'gpt-5-mini']

token_cost_dict = {
    ""
}

def get_token_cost():
    pass

num_generations = 3
for model in model_options:
    for n in num_generations:        
        summary = summarize_paper(paper_contents, model)
        output_score = eval_score(summary, paper_contents, model)
        token_cost = get_token_cost()
        print(f"Model: {model}")
        print(f"Summary: {summary}")
        print(f"Output Score: {output_score}")
        print("\n")
