{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-intro",
      "metadata": {},
      "source": [
        "# GPT-5.2 Prompting Guide\n",
        "\n",
        "This notebook covers prompting techniques specific to GPT-5.2, based on OpenAI's official cookbook.\n",
        "\n",
        "## Key GPT-5.2 Traits\n",
        "\n",
        "| Trait | Description |\n",
        "|-------|-------------|\n",
        "| Deliberate scaffolding | Constructs clearer intermediate plans by default |\n",
        "| Lower verbosity | More concise, task-focused output |\n",
        "| Stronger instruction adherence | Reduced drift from user intent |\n",
        "| Conservative grounding | Favors correctness and explicit reasoning |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-toc",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Setup](#setup)\n",
        "2. [Verbosity & Output Control](#verbosity)\n",
        "3. [Scope Discipline](#scope)\n",
        "4. [Long-Context Handling](#long-context)\n",
        "5. [Hallucination Mitigation](#hallucination)\n",
        "6. [Structured Extraction](#extraction)\n",
        "7. [Tool-Calling Best Practices](#tools)\n",
        "8. [Context Compaction](#compaction)\n",
        "9. [Model Migration](#migration)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-setup-header",
      "metadata": {},
      "source": [
        "<a id='setup'></a>\n",
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client initialized\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "client = OpenAI()\n",
        "print(\"OpenAI client initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-verbosity-header",
      "metadata": {},
      "source": [
        "<a id='verbosity'></a>\n",
        "## 2. Verbosity & Output Control\n",
        "\n",
        "GPT-5.2 responds well to explicit output shape constraints. For enterprise/coding agents, define:\n",
        "- Default response length\n",
        "- Format for simple vs. complex answers\n",
        "- Preference for lists/bullets over narrative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-verbosity-spec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verbosity specification defined\n"
          ]
        }
      ],
      "source": [
        "# Define a reusable verbosity specification\n",
        "VERBOSITY_SPEC = \"\"\"\n",
        "<output_verbosity_spec>\n",
        "- Default: 3-6 sentences or ≤5 bullets for typical answers.\n",
        "- For yes/no questions: ≤2 sentences.\n",
        "- For complex multi-step tasks:\n",
        "  - 1 short overview paragraph\n",
        "  - then ≤5 bullets: What changed, Where, Risks, Next steps, Open questions.\n",
        "- Prefer compact bullets over long narrative paragraphs.\n",
        "- Do not rephrase the user's request unless semantics change.\n",
        "</output_verbosity_spec>\n",
        "\"\"\"\n",
        "\n",
        "print(\"Verbosity specification defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell-verbosity-example",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing output with verbosity control\n",
            "------------------------------------------------------------\n",
            "DEFAULT OUTPUT:\n",
            "REST and GraphQL are two different approaches to designing web APIs. The main differences are about how you model data, fetch it, and evolve the API over time.\n",
            "\n",
            "## 1) Interface and endpoints\n",
            "- **REST:** Many endpoints, typically organized around resources (e.g., `GET /users/123`, `GET /users/123/orders`).\n",
            "- **GraphQL:** Usually a **single endpoint** (e.g., `POST /graphql`) where the client specifies what it needs in the request body.\n",
            "\n",
            "## 2) Data fetching: over-fetching vs under-fetching\n",
            "- **REST:** Responses are fixed per endpoint. Clients often:\n",
            "  - **Over-fetch** (get more fields than needed), or\n",
            "  - **Under-fetch** (need multiple requests to assemble a view).\n",
            "- **GraphQL:** Clients request **exactly the fields** they need in one query, which can reduce both over- and under-fetching.\n",
            "\n",
            "Example:\n",
            "- REST: might require `GET /user/123` and `GET /user/123/orders`\n",
            "- GraphQL: one query:\n",
            "  ```graphql\n",
            "  query {\n",
            "    user(id: \"123\") {\n",
            "      name\n",
            "      orders { id total }\n",
            "    }\n",
            "  }\n",
            "  ```\n",
            "\n",
            "## 3) Schema and typing\n",
            "- **REST:** No required standard schema. Some APIs publish OpenAPI/Swagger specs, but it’s optional.\n",
            "- **GraphQL:** Strongly centered on a **typed schema** (types, fields, relationships). The schema is a contract and enables tooling (validation, auto-completion, introspection).\n",
            "\n",
            "## 4) Versioning and evolution\n",
            "- **REST:** Often uses explicit versioning (e.g., `/v1/...`, `/v2/...`) when making breaking changes.\n",
            "- **GraphQL:** Typically evolves by **adding fields/types** and deprecating old ones, aiming to avoid frequent versioned endpoints. Breaking changes are still possible but are usually managed via deprecation.\n",
            "\n",
            "## 5) Caching\n",
            "- **REST:** Works very naturally with HTTP caching semantics (GET, ETags, `Cache-Control`, CDNs).\n",
            "- **GraphQL:** Caching is more complex because many requests go through one endpoint and are POSTed with arbitrary queries. It often requires:\n",
            "  - client-side normalized caching (e.g., Apollo),\n",
            "  - persisted queries, or\n",
            "  - custom caching layers.\n",
            "\n",
            "## 6) Error handling\n",
            "- **REST:** Uses HTTP status codes heavily (404, 401, 500) and typically returns errors per request.\n",
            "- **GraphQL:** Usually returns **200 OK** even when parts of the query fail, with an `errors` array plus any partial `data` that succeeded. HTTP status codes still matter for transport/auth failures.\n",
            "\n",
            "## 7) Performance and complexity trade-offs\n",
            "- **REST:** Simpler operationally; performance is more predictable per endpoint. But complex views may require multiple round trips.\n",
            "- **GraphQL:** Can reduce round trips, but servers must handle:\n",
            "  - query complexity and depth (to prevent expensive queries),\n",
            "  - N+1 resolver issues (often mitigated with batching like DataLoader),\n",
            "  - careful authorization per field.\n",
            "\n",
            "## 8) Tooling and client experience\n",
            "- **REST:** Broadly supported; easy to call with basic HTTP tools. Discoverability depends on documentation quality (or OpenAPI).\n",
            "- **GraphQL:** Excellent developer experience with schema-driven tooling (GraphiQL/Playground, code generation, typed clients).\n",
            "\n",
            "## When each is a good fit\n",
            "- **REST is a good fit when:** you want simple resource-based CRUD, strong HTTP caching/CDN behavior, and straightforward ops.\n",
            "- **GraphQL is a good fit when:** clients (web/mobile) need flexible shapes of data, you have many related entities, and you want a single, strongly-typed contract.\n",
            "\n",
            "If you share your use case (public API vs internal, number of clients, caching needs), I can recommend which approach fits better.\n",
            "\n",
            "Tokens: 815\n",
            "\n",
            "============================================================\n",
            "\n",
            "WITH VERBOSITY SPEC:\n",
            "REST and GraphQL are two different styles for designing APIs, mainly differing in how clients request data and how endpoints are structured.\n",
            "\n",
            "- **API shape (endpoints vs single endpoint):** REST typically exposes many endpoints (e.g., `/users`, `/users/{id}/posts`), each returning a fixed resource shape. GraphQL usually exposes a **single endpoint** where clients send queries that specify exactly what fields they want.\n",
            "- **Data fetching (fixed responses vs client-specified):** REST responses are determined by the server per endpoint, which can lead to **over-fetching** (too much data) or **under-fetching** (not enough, requiring more calls). GraphQL lets clients request precisely the needed fields, often reducing both issues.\n",
            "- **Versioning and evolution:** REST often uses explicit versioning (e.g., `/v1/...`) when response shapes change. GraphQL encourages evolving the schema by adding fields and **deprecating** old ones, so clients can migrate gradually.\n",
            "- **Caching behavior:** REST works naturally with HTTP caching semantics (URLs map to resources, leveraging `GET` and cache headers). GraphQL can be cached, but it often requires more custom strategies (e.g., persisted queries, normalized client caches) because many requests hit the same endpoint.\n",
            "- **Error handling and tooling:** REST relies on HTTP status codes plus response bodies; GraphQL typically returns `200 OK` with an `errors` array for field-level failures, alongside partial `data`. GraphQL also provides strong schema introspection and type systems, enabling powerful tooling and auto-generated documentation.\n",
            "\n",
            "Tokens: 325\n"
          ]
        }
      ],
      "source": [
        "# Compare responses with and without verbosity spec\n",
        "print(\"Comparing output with verbosity control\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "question = \"Explain the differences between REST and GraphQL APIs.\"\n",
        "\n",
        "# Without verbosity spec\n",
        "response_default = client.responses.create(\n",
        "    model=\"gpt-5.2\",\n",
        "    reasoning={\"effort\": \"low\"},\n",
        "    input=[{\"role\": \"user\", \"content\": question}]\n",
        ")\n",
        "\n",
        "# With verbosity spec\n",
        "response_controlled = client.responses.create(\n",
        "    model=\"gpt-5.2\",\n",
        "    reasoning={\"effort\": \"low\"},\n",
        "    input=[\n",
        "        {\"role\": \"developer\", \"content\": VERBOSITY_SPEC},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"DEFAULT OUTPUT:\")\n",
        "print(response_default.output_text)\n",
        "print(f\"\\nTokens: {response_default.usage.output_tokens}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\\nWITH VERBOSITY SPEC:\")\n",
        "print(response_controlled.output_text)\n",
        "print(f\"\\nTokens: {response_controlled.usage.output_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-scope-header",
      "metadata": {},
      "source": [
        "<a id='scope'></a>\n",
        "## 3. Scope Discipline\n",
        "\n",
        "Prevent GPT-5.2 from adding unintended features or embellishments, especially in frontend/design work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-scope-spec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scope discipline example\n",
            "------------------------------------------------------------\n",
            "```python\n",
            "def add_two_numbers(a, b):\n",
            "    return a + b\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "SCOPE_SPEC = \"\"\"\n",
        "<design_and_scope_constraints>\n",
        "- Implement EXACTLY and ONLY what the user requests.\n",
        "- No extra features, components, or UX embellishments.\n",
        "- Do NOT invent colors, shadows, animations, or new UI elements.\n",
        "- If any instruction is ambiguous, choose the simplest valid interpretation.\n",
        "</design_and_scope_constraints>\n",
        "\"\"\"\n",
        "\n",
        "print(\"Scope discipline example\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "task = \"Create a Python function that adds two numbers.\"\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5.2\",\n",
        "    reasoning={\"effort\": \"none\"},\n",
        "    input=[\n",
        "        {\"role\": \"developer\", \"content\": SCOPE_SPEC},\n",
        "        {\"role\": \"user\", \"content\": task}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-long-context-header",
      "metadata": {},
      "source": [
        "<a id='long-context'></a>\n",
        "## 4. Long-Context Handling\n",
        "\n",
        "For inputs >10K tokens, use forced summarization and re-grounding to user constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-long-context-spec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Long-context handling pattern\n",
            "------------------------------------------------------------\n",
            "Based on the **Contract Summary** you provided, the only termination condition stated is:\n",
            "\n",
            "- **“Termination Notice: 90 days written notice required”**\n",
            "\n",
            "No separate **termination fee/penalty** is mentioned in the summary, so **the only “without penalty” path we can safely infer from what you shared is terminating by giving the required 90 days’ written notice** (subject to anything else that may exist in the full contract).\n",
            "\n",
            "### To avoid auto-renewal / terminate at the end of the initial term\n",
            "- **Effective Date:** January 1, 2025  \n",
            "- **Term:** 24 months → runs through **December 31, 2026**\n",
            "- To end it at that point, you must deliver written notice **at least 90 days before 12/31/2026**, i.e. **by October 2, 2026**.\n",
            "\n",
            "### For later renewals\n",
            "Because it **auto-renews in 12-month periods**, you’d need to give notice **90 days before the end of any renewal period** (e.g., to end on **12/31/2027**, notice would be due **by 10/2/2027**, etc.).\n",
            "\n",
            "### Important caveat\n",
            "The summary does **not** say whether termination is allowed **only at term end** vs. **at any time with 90 days’ notice**, and it doesn’t mention any **early termination charges**. If you want, paste the termination clause from the full contract and I’ll pin down whether you can terminate mid-term and whether any penalties apply.\n"
          ]
        }
      ],
      "source": [
        "LONG_CONTEXT_SPEC = \"\"\"\n",
        "<long_context_handling>\n",
        "- For lengthy inputs (multi-chapter docs, long threads, multiple files):\n",
        "  1. First, produce a short internal outline of key sections relevant to the request.\n",
        "  2. Re-state user's constraints explicitly (date range, product, team, etc.).\n",
        "  3. Anchor claims to specific sections rather than speaking generically.\n",
        "- If the answer depends on fine details (dates, thresholds, clauses), quote them.\n",
        "</long_context_handling>\n",
        "\"\"\"\n",
        "\n",
        "print(\"Long-context handling pattern\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Simulating a document analysis task\n",
        "document = \"\"\"\n",
        "=== CONTRACT SUMMARY ===\n",
        "Effective Date: January 1, 2025\n",
        "Parties: TechCorp Inc. and DataServices LLC\n",
        "Term: 24 months\n",
        "Auto-Renewal: Yes, 12-month periods\n",
        "Termination Notice: 90 days written notice required\n",
        "Liability Cap: $500,000\n",
        "Governing Law: State of Delaware\n",
        "\"\"\"\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5.2\",\n",
        "    reasoning={\"effort\": \"medium\"},\n",
        "    input=[\n",
        "        {\"role\": \"developer\", \"content\": LONG_CONTEXT_SPEC},\n",
        "        {\"role\": \"user\", \"content\": f\"Document:\\n{document}\\n\\nQuestion: When can we terminate without penalty?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-hallucination-header",
      "metadata": {},
      "source": [
        "<a id='hallucination'></a>\n",
        "## 5. Hallucination Mitigation\n",
        "\n",
        "Configure prompts to handle uncertain or underspecified queries safely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cell-hallucination-spec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hallucination mitigation example\n",
            "------------------------------------------------------------\n",
            "I can’t determine “Acme Corp’s exact revenue last quarter” from here without a specific source, because there are many companies that could be called “Acme Corp,” and revenue depends on the reporting entity and period.\n",
            "\n",
            "1) Which Acme Corp do you mean (ticker/website/country)?  \n",
            "2) What do you mean by “last quarter” (calendar Q4 2025, most recent fiscal quarter, or a specific quarter-end date)?  \n",
            "3) Do you want the GAAP revenue figure from an earnings release/10‑Q, or another definition (e.g., net sales, operating revenue)?\n",
            "\n",
            "If you share a link to the earnings report/filing (or paste the relevant table), I can extract the exact revenue number and cite where it appears.\n"
          ]
        }
      ],
      "source": [
        "UNCERTAINTY_SPEC = \"\"\"\n",
        "<uncertainty_and_ambiguity>\n",
        "- If the question is ambiguous or underspecified:\n",
        "  - Ask 1-3 precise clarifying questions, OR\n",
        "  - Present 2-3 plausible interpretations with labeled assumptions.\n",
        "- When external facts may have changed recently:\n",
        "  - Answer in general terms and state that details may have changed.\n",
        "- Never fabricate exact figures, line numbers, or external references when uncertain.\n",
        "- When unsure, prefer language like \"Based on the provided context...\"\n",
        "</uncertainty_and_ambiguity>\n",
        "\"\"\"\n",
        "\n",
        "# Optional high-risk self-check for legal/financial contexts\n",
        "HIGH_RISK_SPEC = \"\"\"\n",
        "<high_risk_self_check>\n",
        "Before finalizing answers in legal, financial, compliance, or safety contexts:\n",
        "- Re-scan your answer for unstated assumptions\n",
        "- Check for specific numbers or claims not grounded in context\n",
        "- Identify overly strong language (\"always,\" \"guaranteed,\" etc.)\n",
        "- If found, soften or qualify them and explicitly state assumptions.\n",
        "</high_risk_self_check>\n",
        "\"\"\"\n",
        "\n",
        "print(\"Hallucination mitigation example\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5.2\",\n",
        "    reasoning={\"effort\": \"medium\"},\n",
        "    input=[\n",
        "        {\"role\": \"developer\", \"content\": UNCERTAINTY_SPEC},\n",
        "        {\"role\": \"user\", \"content\": \"What's the exact revenue of Acme Corp last quarter?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-extraction-header",
      "metadata": {},
      "source": [
        "<a id='extraction'></a>\n",
        "## 6. Structured Extraction\n",
        "\n",
        "For PDF/table extraction, provide explicit JSON schemas with required vs. optional field distinctions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cell-extraction-example",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structured extraction example\n",
            "------------------------------------------------------------\n",
            "Party: GlobalTech Solutions\n",
            "Effective Date: March 15, 2025\n",
            "Jurisdiction: None\n",
            "Termination: Either party may terminate with 60 days written notice.\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "class ContractExtraction(BaseModel):\n",
        "    \"\"\"Schema for contract data extraction\"\"\"\n",
        "    party_name: str = Field(description=\"Name of the contracting party\")\n",
        "    jurisdiction: Optional[str] = Field(default=None, description=\"Legal jurisdiction if specified\")\n",
        "    effective_date: Optional[str] = Field(default=None, description=\"Contract effective date\")\n",
        "    termination_clause: Optional[str] = Field(default=None, description=\"Summary of termination terms\")\n",
        "\n",
        "print(\"Structured extraction example\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "contract_text = \"\"\"\n",
        "This Agreement is entered into by GlobalTech Solutions, effective March 15, 2025.\n",
        "Either party may terminate with 60 days written notice.\n",
        "\"\"\"\n",
        "\n",
        "response = client.responses.parse(\n",
        "    model=\"gpt-5.2\",\n",
        "    reasoning={\"effort\": \"medium\"},\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"developer\",\n",
        "            \"content\": \"Extract contract data. If a field is not present, set it to null.\"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": f\"Extract from:\\n{contract_text}\"}\n",
        "    ],\n",
        "    text_format=ContractExtraction\n",
        ")\n",
        "\n",
        "result = response.output_parsed\n",
        "print(f\"Party: {result.party_name}\")\n",
        "print(f\"Effective Date: {result.effective_date}\")\n",
        "print(f\"Jurisdiction: {result.jurisdiction}\")\n",
        "print(f\"Termination: {result.termination_clause}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-tools-header",
      "metadata": {},
      "source": [
        "<a id='tools'></a>\n",
        "## 7. Tool-Calling Best Practices\n",
        "\n",
        "GPT-5.2 tool usage guidelines:\n",
        "- Prefer tools over internal knowledge for fresh/user-specific data\n",
        "- Parallelize independent reads\n",
        "- After write operations, restate what changed and where"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cell-tools-spec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool-calling with GPT-5.2 (OpenAI function calling style)\n",
            "------------------------------------------------------------\n",
            "Parsed tool function call:\n",
            "  Name: get_user_orders\n",
            "  Call ID: call_RLSxNUN28ymoIkr0vmTUVRKJ\n",
            "  Arguments: {\"user_id\":\"U12345\",\"status\":\"pending\"}\n"
          ]
        }
      ],
      "source": [
        "TOOL_USAGE_SPEC = \"\"\"\n",
        "<tool_usage_rules>\n",
        "- Prefer tools over internal knowledge when:\n",
        "  - You need fresh or user-specific data (tickets, orders, configs, logs).\n",
        "  - You reference specific IDs, URLs, or document titles.\n",
        "- Parallelize independent reads when possible to reduce latency.\n",
        "- After any write/update tool call, briefly restate:\n",
        "  - What changed\n",
        "  - Where (ID or path)\n",
        "  - Any follow-up validation performed.\n",
        "</tool_usage_rules>\n",
        "\"\"\"\n",
        "\n",
        "print(\"Tool-calling with GPT-5.2 (OpenAI function calling style)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Note: Each tool object MUST include a \"name\" key at the top level of the tool dictionary (per OpenAI API)\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"name\": \"get_user_orders\",  # <-- Required top-level attribute\n",
        "        \"description\": \"Retrieve orders for a user by their ID\",\n",
        "        \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"user_id\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The unique identifier for the user whose orders are to be retrieved\"\n",
        "                    },\n",
        "                    \"status\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"pending\", \"shipped\", \"delivered\"],\n",
        "                        \"description\": \"The status of orders to filter by. One of: pending, shipped, delivered.\"\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"user_id\"]\n",
        "            }\n",
        "        }\n",
        "]\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5.2\",\n",
        "    reasoning={\"effort\": \"low\"},\n",
        "    tools=tools,\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": TOOL_USAGE_SPEC},\n",
        "        {\"role\": \"user\", \"content\": \"What are the pending orders for user U12345?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Output: the assistant may provide a direct answer or request a tool call via function_call type\n",
        "# Simulate parsing tool call output to extract function name and call ID\n",
        "tool_call = response.output[-1]\n",
        "\n",
        "print(\"Parsed tool function call:\")\n",
        "print(f\"  Name: {tool_call.name}\")\n",
        "print(f\"  Call ID: {tool_call.call_id}\")\n",
        "print(f\"  Arguments: {tool_call.arguments}\")\n",
        "# Should show: get_user_orders and the correct call ID and arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-compaction-header",
      "metadata": {},
      "source": [
        "<a id='compaction'></a>\n",
        "## 8. Context Compaction\n",
        "\n",
        "For multi-step agent flows exceeding context limits, use the compaction endpoint to reduce token footprint while preserving essential information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "505bb635",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context Compaction Example\n",
            "------------------------------------------------------------\n",
            "Original response tokens: 3388\n",
            "\n",
            "Compacted response:\n",
            "{\n",
            "  \"id\": \"resp_04d3722d907f4ab20169849e22bfdc819497e7c118202d9aeb\",\n",
            "  \"created_at\": 1770298924,\n",
            "  \"object\": \"response.compaction\",\n",
            "  \"output\": [\n",
            "    {\n",
            "      \"id\": \"msg_04d3722d907f4ab20169849e22c71c8194916ca1ae2a09e9d9\",\n",
            "      \"content\": [\n",
            "        {\n",
            "          \"annotations\": null,\n",
            "          \"text\": \"write a very long poem about a dog.\",\n",
            "          \"type\": \"input_text\",\n",
            "          \"logprobs\": null\n",
            "        }\n",
            "      ],\n",
            "      \"role\": \"user\",\n",
            "      \"status\": \"completed\",\n",
            "      \"type\": \"message\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"cmp_04d3722d907f4ab20169849e23757c8194aa2b9d37e21a61ad\",\n",
            "      \"encrypted_content\": \"gAAAAABphJ4sT0QEPlhPLmgxgCO2IHQYmgB_PAqlPrRS9Lb9ISKVwTYR-h_FbGv7WmwoV9d4GQfzGrlHXrOLX9Etd_rIKyxwKA7FXTxWhFTGDcQOauBohYnW1Q-ZrahPf3F9vGybQtCvmnbR9U2B-M2cLMDiwbg7knUnUmVQ2NCCTqlFVZrdZ3QQFWG6G6wcOmCjiiuZ5H9wkdp8alVPmNa-KlcU5x76L9-quU4bJNS-5DRYrCNiXmVP1hpip96yupreEFhNhxIxg_0ShnDb8ZiI9fd4fRK1yYML4Hb6woRLnEOJxFaMIPkOJIAp2qgsqd9eHIZrqQ899SeQYgQUa1XBZIw1PLI5AjIMsXfrO3KZgMGfcXlsHO0pk7x81_bQcYycr2Zi2G3wh-byuNr8dGC5WOoWfjGqBc3tLjJcJkbyfhyQbm8vXf4R9eV0Xb2L4IzqCJZKR9ktIi3Q6HHh35QlFXXhT1wAXXbL0Bm6V5mwKp2WOTycZ9AWiyVSIJoIVbla31CUH0rdfalxRaBYYbmcuccP4IOKSnFm2dHDBXa4vhkm4J1hCYROabsjvXuFHcTa43896EEt4M4cjYyUn0_jDxkEbDkubxQBJxcy9eWRh2eBKhiVCPh5w2-UA9ToJ36zqmbJiCTe1f93Kzslr8cifBjsutwlcOy9gusEF4Jrv9Akxueccbvvvtv1LzgGyVJ4P_6G7FFg4Yag4bmg09nipsbRivJWTLBEvHY-HicYXh5sx-Pymi3FjebMB07_7vtEcGta5JWOt8G-10L2as1h85BMHngV337XGDsawDxzQi0rytfTAzw00QeA4hUCScr9MAIQUyI3EBV-rWhxg8mkO54_fUv5oeJhSmEWgegTIGsDyPNmw9Qkquiv5UuCgCdSv9Hm9jTPg07VL3DFMrX8Iu3c3k9o6OU82rEw7NFpS8YO9FCEOvp2QuKmI0e-e7dDWuHcHrYJgm5beWoz8elyZgqpvQMtU4WmLttWBE9NIAhSlxeO-PkCXNsGzfNX1DqT_iV8E3Ja3PPt_XMfrqutSNrE5bew_8KjDBHrR1QQMPSd7V9kC61WvKFRaRp6a_YzVeJ1-K5CK-dHhIOsXN-TgaqMyRn00GF_gFm7MfDQgqrB6J_fdsAMvtfEauh37hTYjPTl0sk4asH76zF5S-3B4zs8yNsHdbT1eM3OsvI6wNPjEOHBruYO9ef8jq8NuBg4nrwvVX1ecmF5GNnfxdre1MP5565wY4M6NH8uF9dyc6RH0kyYss7Q_ZVsd7TnrpJ5zX4ioazqoKZYEjFuz6lFdGggBi0r47Ah8SbPsxZY21elqjOJr5Ahg_MoZ7DLqTZcUNm6MM_u7IiJVc6ev8pHReCw-iDh9horql8TCx2vfUl0wNaME50DlM0z7zvooET59N4usPY105rlSpGQ5YYrnkDLd4a6KpVJ_7BDjcX1ivR87BeKb3_-TdFJoMeiIuBbVbe15t1fc36hU1iPYVs3X6Y6mzlW5oiN4MQ7cXHHubBRx6Umo2Txo062sxbvNBfXW2Q133xeRttv4cww_Yv2am1Tk6CxAjh9VwwgbugOQ3Skm1usvMuNBL0Q9V1bDKV8angwqol0ODipERoz5TdlPJDcGkurlY5stZfww0Hk5aWDTQCQktesH2ha8C_d10RiEas_FH91TqPrCWxiJ271z3zJ72GIUGW9nX2fAo-6JIP7khzN5mpX3dJjGLbRhJy5oAdvD6JJ6pbee-xvUxIZzwlSET2zLqHGyMNt2ZzBVcQCv5ckRNBZ4jBV_bHeLyIDEdR3U7tfQ_nw6B11DiHSug73YfoHVDiDnw3gRvebodJ6XURFEz9kXHmqpCNjwgddwg0rIoR9PwlZbbrLPmm8tzcBfchkYxtvpz-PYXRckvWxgwL9xeb8WyQVqZ0Or8SifV1JltRmDO8X-ndkKbKQhGqdCfvBojWCZ_C8reduUSbKzlLnzCPThtwEevgxh-QSu64AQTtSdH-OMlUXX50YkcA2JTWrfaefTkjoxx-JOhl0nNv6_3SkfVYrjrRrrD200Dp1twUnFJzUZz55aMy2RO6vQvdKfNKpVmftqWrgF0dWgNNqYXiA-T6LGQlSPm_jsXg0dSgpPglmKnabTmzQ9DJx8hH0sZ21-fh6m4uALj2hMBd-DBq_tlKsnQOfoivfDb2jkkswMqxvLgZ3tu83kWf5Izc4wvM3HeDo-10P9zNKt6W45xqmRHWnkyz9QpnxZ_igFSO3FKkEtCVZ_rpikoPMZnlTRpwTbjNbbTFzLi-POPleS3Lqs8djACN4CpSgKYPpOgZ4U2gCZ4sFjYQ3i0y_nA7e4BbEGgfNAAeJFn1aanbu0TNYMJwfazkj5GlFtjf1fleUG0WsAl_n3U1-Wjf_ebkTO3WHuG6pji39m6z6gPa_b0OWjIEcGTy7-kfbkB3yKEC-WgtZRhNUh_ieBJl6E4fOmwpOf3kwuoEKavbMyi9WmQNVI8Lu94WKJACqwtyaWRXkHjIaYzHEYvu3JSSefhbZ6Gavy2GhfkF6RVKihQaYuY5ojv15du9DqPKI_Qw4h_m_ND93UFScs8gDlz75ewq8ToLSO6LPMIN7u0Zo3_TRrAA4fH7Hla5bAAKQ0MZI74jZ75x-BIuv0XbQDiH9rnrf2opk9NRTN_Lo_N4bjQnAQTHMcnBc6KuxBn1YYvXBihTT3E3Us8XU4lbawOMdeHNkhRMqqRcUeWCNiIFAL6wbvQxajRG-0JDPwMzTaSAD4rgqmJVfkIJgEvCNP2iAP-OXyIInhmbGyLQ_Kw1NKWkJ7xw0t7H56RhMsZN2kHR8-cpy7SdQnRTxZDVke0zc_7uziUyeDnWq-253V0p_sAkdgPEyhTROz0B27sPGJ3ItP7_mJyKYLZZTSVlQIhcm3ZJ_6deNcOZGTbr2ANLi4MQ0Kd2UbgeEQqvaH1z0AP6LINLaES7AO9V6Q-qBERBv87Vlhw80ckjuKno3tEpgWxJ2AZ4fnYktZe7I__U_vAFwkZa3zfnyE7lHtz46nYcfWa6EQ1d19tih-dAX3m5XJPngWstXRZyWQg7izNoNnAxjEwOhgio-1DN350FxDz_-4mpur0jvrBYWGY8Z39WMoofkXfGWIt3VGwMtsWy0FRsmV-I8f5dPRvEqJ61aPLdoQAOg9NrUHNIjMP6B9LHop2MjP4793pSCF4Sz-BkfiGftUoL4_uf8tlyHApIInYd3pP83tiHmjgv4eI2YUwfD-mwSL0jM1uI3UOkrl6bXnksztjsWlrOEWNgAhQfj014ze5fnN7Quzv-gShrPxh_cjsg85Ps8j4AWf1P4JWdzP6BZUuGg69oG94r68WUtiBM-m0uBbdch76dEJfx1-t0BAtSrXjKJ_QmA88x0O_5g1Vj4ahbQ1UJ7cuc8wLz5w80WbwpQhsmV-mDm-2J7EMRUlTDH5n5Vr5j-2wpJUvREBEf3pkB5PaEGXK0SHHoL_SG5oi80PwdMITelmN9R8KYt-AKhGoppkz-6JkK6JeTgUJ6ndJWYi7mK1kGbWQkuJwJ3YtLoREaDhh9yl5k-o-VHyI5C8yBaO1tq9Hf74paAyHIFrzlRoURoksiSqso2LpbdOgxvocLcgpzTneFa45YhKyguKzi01n0gzoGCRaa3ZfRPpD-o1gb8IwqWXwp7yIpTzFf5yKzPninRVvEV4Tf9_a84tYfsJcmM8QWjSUh4SKTZDPfQ4M9ekP_3ILLDa5vQfXviNqeEkLVab_B4gshHKizc6wIkhfWjvEJRAg4If0Tp_V4v0dnEvGQ6qpEpFOQPAeVBdAYuNu8_PpQqqOVTlbqkCJ9jioKwYCGizglrIesIJb5-XM4XHyH536-F8-Gh0cejkxV2aVOUvm9iFE0YOF5_0BE9-DNXLo5c8J8q3rO5OLZPDSWDAiMJD8j0Jm-nBgbDP1Xt-vkgXLqziwi6MWjhXg33ds_BYo_5S665_rITirRc2tzLzCfe4OzWsHfZoNFjp61xV7DnlmCXvQU5aTef6sWVV_2q9srfT5Y4rSS9ujgJlIBVVnEJrtTFxzq9UPKR23B3tkBjsr9dr9CS6YMalUQdCVwAT0xuZF41HBXLqTlTgHpo-lH3Jnb0IZO7UWr06fd9SKhvYZ5wP2qkudIyVDD0kuPRh8S2NBSV97KPBxuuBXMsqgCsHJlW_ES_o0K2rd3NFaBAtFRtZ6uZlNyustqTSvp5_7ZZiiHVaM-wRiTvrTnMgb7Qwkryy45glNh5O8Hj4bh6kxyQughJLcghwear7mYyB2eN_yFjvqTHITWj\",\n",
            "      \"type\": \"compaction\",\n",
            "      \"created_by\": null\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"input_tokens\": 3498,\n",
            "    \"input_tokens_details\": {\n",
            "      \"cached_tokens\": 3200\n",
            "    },\n",
            "    \"output_tokens\": 456,\n",
            "    \"output_tokens_details\": {\n",
            "      \"reasoning_tokens\": 0\n",
            "    },\n",
            "    \"total_tokens\": 3954\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(\"Context Compaction Example\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Step 1: Generate a response that we'll then compact\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5.2\",\n",
        "    input=\"write a very long poem about a dog.\"\n",
        ")\n",
        "\n",
        "print(f\"Original response tokens: {response.usage.output_tokens}\")\n",
        "\n",
        "# Get the output as a dict for compaction\n",
        "output_item = response.output[0].model_dump()\n",
        "\n",
        "# Step 2: Compact the conversation\n",
        "# Note: client.responses.compact() requires openai>=1.60.0\n",
        "# The API endpoint is POST /v1/responses/compact\n",
        "try:\n",
        "    compacted_response = client.responses.compact(\n",
        "        model=\"gpt-5.2\",\n",
        "        input=[\n",
        "            {\"role\": \"user\", \"content\": \"write a very long poem about a dog.\"},\n",
        "            output_item\n",
        "        ]\n",
        "    )\n",
        "    print(\"\\nCompacted response:\")\n",
        "    print(json.dumps(compacted_response.model_dump(), indent=2))\n",
        "except AttributeError:\n",
        "    print(\"\\nNote: client.responses.compact() not available in your SDK version.\")\n",
        "    print(\"Update with: pip install --upgrade openai\")\n",
        "    print(\"\\nAlternatively, use the REST API directly:\")\n",
        "    print(\"POST https://api.openai.com/v1/responses/compact\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-migration-header",
      "metadata": {},
      "source": [
        "<a id='migration'></a>\n",
        "## 9. Model Migration Guide\n",
        "\n",
        "When upgrading to GPT-5.2 from earlier models:\n",
        "\n",
        "| From | reasoning_effort | Notes |\n",
        "|------|-----------------|-------|\n",
        "| GPT-4o | `none` | Default to fast/low-deliberation |\n",
        "| GPT-4.1 | `none` | Preserve snappy behavior |\n",
        "| GPT-5 | same | Maintain latency/quality profile |\n",
        "| GPT-5.1 | same | Adjust only after running evals |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-migration-steps",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Migration Steps\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "migration_steps = \"\"\"\n",
        "1. Switch models without changing prompts\n",
        "2. Explicitly set reasoning_effort to match prior model's profile\n",
        "3. Run evaluation suite for baseline\n",
        "4. Address regressions through targeted prompt tuning\n",
        "5. Re-measure after each incremental change\n",
        "\"\"\"\n",
        "\n",
        "print(migration_steps)\n",
        "\n",
        "# Example: Migrating from GPT-4o\n",
        "print(\"\\nExample: GPT-4o → GPT-5.2 migration\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5.2\",\n",
        "    reasoning={\"effort\": \"none\"},  # Match GPT-4o speed profile\n",
        "    input=[{\"role\": \"user\", \"content\": \"Classify this as spam or not spam: 'You won $1000000!'\"}]\n",
        ")\n",
        "\n",
        "print(f\"Result: {response.output_text}\")\n",
        "print(f\"Total tokens: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-summary",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key GPT-5.2 Prompting Techniques\n",
        "\n",
        "1. **Verbosity Control** - Define explicit output shape constraints\n",
        "2. **Scope Discipline** - Prevent feature creep with explicit boundaries\n",
        "3. **Long-Context** - Force summarization and re-grounding for large inputs\n",
        "4. **Hallucination Mitigation** - Handle uncertainty explicitly\n",
        "5. **Structured Extraction** - Use JSON schemas with null handling\n",
        "6. **Tool Usage** - Parallelize reads, verify writes\n",
        "7. **Compaction** - Reduce context at milestones\n",
        "\n",
        "### Quick Reference\n",
        "\n",
        "| Technique | When to Use |\n",
        "|-----------|-------------|\n",
        "| `VERBOSITY_SPEC` | Enterprise agents, coding assistants |\n",
        "| `SCOPE_SPEC` | Frontend/design work, code generation |\n",
        "| `LONG_CONTEXT_SPEC` | Documents >10K tokens |\n",
        "| `UNCERTAINTY_SPEC` | Ambiguous queries, recent facts |\n",
        "| `HIGH_RISK_SPEC` | Legal, financial, compliance contexts |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
