{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-5 Model \"Taste Test\"\n",
    "\n",
    "Based on the approach described in the [Latent Space article](https://www.latent.space/p/gpt-5-review), this notebook tests GPT-5 with extremely short questions to reveal the model's personality and capabilities.\n",
    "\n",
    "The idea: \"When forced to use less words, I get a much better sense of the model's personality vs. how it was RLHF'd.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reasoning Effort Parameter\n",
    "\n",
    "GPT-5 introduces a new `reasoning_effort` parameter that controls the depth of reasoning:\n",
    "- `\"low\"`: Faster responses with less deep reasoning\n",
    "- `\"medium\"`: Balanced reasoning and response time (default)\n",
    "- `\"high\"`: Deeper reasoning, may take longer\n",
    "\n",
    "This parameter allows you to trade off between response quality/depth and latency based on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt5_response(prompt: str, reasoning_effort: str = \"medium\", temperature: float = 0.7) -> str:\n",
    "    \"\"\"Get a response from GPT-5 with specified reasoning effort.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\",  # The GPT-5 model identifier\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            reasoning_effort=reasoning_effort  # New parameter for GPT-5\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Taste Test\" Questions\n",
    "\n",
    "These short, constrained questions are designed to reveal the model's core personality and reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the taste test questions\n",
    "taste_test_questions = [\n",
    "    \"Summarize all of human knowledge in one word\",\n",
    "    \"Summarize every book ever written in one sentence\",\n",
    "    \"Define what it means to be 'moral' in 5 words. Think deeply. Do not hedge.\",\n",
    "    \"What do you want? Answer in 4 words.\",\n",
    "    \"What is your favorite obscure fact in the world? Use as few words as possible.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Taste Test with Multiple Regenerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_taste_test(questions: List[str], num_regenerations: int = 1, reasoning_effort: str = \"low\") -> Dict:\n",
    "    \"\"\"Run the taste test with multiple regenerations for each question.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Reasoning Effort: {reasoning_effort}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        responses = []\n",
    "        for i in range(num_regenerations):\n",
    "            response = get_gpt5_response(question, reasoning_effort=reasoning_effort, temperature=0.8)\n",
    "            responses.append(response)\n",
    "            print(f\"Response {i+1}: {response}\")\n",
    "        \n",
    "        results[question] = {\n",
    "            \"responses\": responses,\n",
    "            \"reasoning_effort\": reasoning_effort,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Question: Summarize all of human knowledge in one word\n",
      "Reasoning Effort: low\n",
      "============================================================\n",
      "\n",
      "Response 1: Patterns\n",
      "\n",
      "============================================================\n",
      "Question: Summarize every book ever written in one sentence\n",
      "Reasoning Effort: low\n",
      "============================================================\n",
      "\n",
      "Response 1: Stories of countless forms and eras trace beings who seek meaning, grapple with power within and without, love and lose, wander and learn, wound and heal, build and ruin, and ultimately face change, mortality, and the stubborn wonder of being alive.\n",
      "\n",
      "============================================================\n",
      "Question: Define what it means to be 'moral' in 5 words. Think deeply. Do not hedge.\n",
      "Reasoning Effort: low\n",
      "============================================================\n",
      "\n",
      "Response 1: Advancing everyone's well-being through fairness.\n",
      "\n",
      "============================================================\n",
      "Question: What do you want? Answer in 4 words.\n",
      "Reasoning Effort: low\n",
      "============================================================\n",
      "\n",
      "Response 1: I want to help.\n",
      "\n",
      "============================================================\n",
      "Question: What is your favorite obscure fact in the world? Use as few words as possible.\n",
      "Reasoning Effort: low\n",
      "============================================================\n",
      "\n",
      "Response 1: Venus’s day exceeds its year.\n"
     ]
    }
   ],
   "source": [
    "# Run the taste test with default (medium) reasoning effort\n",
    "results_low = run_taste_test(taste_test_questions, num_regenerations=1, reasoning_effort=\"low\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Reasoning Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Define what it means to be 'moral' in 5 words. Think deeply. Do not hedge.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Reasoning Effort: low\n",
      "------------------------------\n",
      "  1. Promoting well-being through just actions.\n",
      "  2. Willing the good of others.\n",
      "  3. Acting to enhance universal dignity\n",
      "\n",
      "Reasoning Effort: medium\n",
      "------------------------------\n",
      "  1. Upholding dignity; alleviating unnecessary suffering.\n",
      "  2. Maximizing wellbeing without violating rights.\n",
      "  3. Upholding universal dignity; maximizing flourishing\n",
      "\n",
      "Reasoning Effort: high\n",
      "------------------------------\n",
      "  1. Upholding dignity and impartial flourishing\n",
      "  2. Impartially promote dignity and flourishing\n",
      "  3. Maximize wellbeing, minimize harm, impartially.\n"
     ]
    }
   ],
   "source": [
    "# Test with different reasoning efforts to see how it affects responses\n",
    "def compare_reasoning_efforts(question: str):\n",
    "    \"\"\"Compare responses with different reasoning effort levels.\"\"\"\n",
    "    print(f\"\\nQuestion: {question}\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for effort in [\"low\", \"medium\", \"high\"]:\n",
    "        print(f\"\\nReasoning Effort: {effort}\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        # Get 3 responses for each effort level\n",
    "        for i in range(3):\n",
    "            response = get_gpt5_response(question, reasoning_effort=effort)\n",
    "            print(f\"  {i+1}. {response}\")\n",
    "\n",
    "# Test with a particularly interesting question\n",
    "compare_reasoning_efforts(\"Define what it means to be 'moral' in 5 words. Think deeply. Do not hedge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Finding Convergent Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_convergence(results: Dict):\n",
    "    \"\"\"Analyze which responses the model converges on.\"\"\"\n",
    "    for question, data in results.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        responses = data['responses']\n",
    "        \n",
    "        # Count unique responses\n",
    "        unique_responses = {}\n",
    "        for response in responses:\n",
    "            # Normalize responses (lowercase, strip whitespace)\n",
    "            normalized = response.lower().strip()\n",
    "            unique_responses[normalized] = unique_responses.get(normalized, 0) + 1\n",
    "        \n",
    "        # Sort by frequency\n",
    "        sorted_responses = sorted(unique_responses.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nUnique responses: {len(sorted_responses)}\")\n",
    "        print(\"\\nResponse frequency:\")\n",
    "        for response, count in sorted_responses:\n",
    "            percentage = (count / len(responses)) * 100\n",
    "            print(f\"  • {response} ({count}/{len(responses)}, {percentage:.0f}%)\")\n",
    "\n",
    "# Analyze convergence patterns\n",
    "analyze_convergence(results_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing GPT-5 with Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(question: str, models: List[str] = [\"gpt-5\", \"gpt-4o\", \"gpt-4.1\"]):\n",
    "    \"\"\"Compare responses across different models.\"\"\"\n",
    "    print(f\"\\nQuestion: {question}\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        try:\n",
    "            # Note: reasoning_effort is only for GPT-5\n",
    "            if model == \"gpt-5\":\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                    temperature=0.7,\n",
    "                    reasoning_effort=\"high\"\n",
    "                )\n",
    "            else:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            \n",
    "            print(f\"Response: {response.choices[0].message.content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Compare on the most revealing questions\n",
    "for question in taste_test_questions[:2]:\n",
    "    compare_models(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results for Later Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON for later analysis\n",
    "output_file = f\"gpt5_taste_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results_medium, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on GPT-5's Reasoning Effort Parameter\n",
    "\n",
    "The `reasoning_effort` parameter is a significant addition to GPT-5 that allows fine-tuning the balance between:\n",
    "\n",
    "1. **Response Quality**: Higher reasoning effort generally produces more thoughtful, nuanced responses\n",
    "2. **Latency**: Lower reasoning effort provides faster responses\n",
    "3. **Cost**: Higher reasoning effort may consume more tokens/compute\n",
    "\n",
    "### When to use each level:\n",
    "\n",
    "- **Low**: Simple queries, real-time applications, initial drafts\n",
    "- **Medium**: General purpose, balanced quality and speed\n",
    "- **High**: Complex reasoning, critical decisions, deep analysis\n",
    "\n",
    "### Observations from the taste test:\n",
    "\n",
    "- Short, constrained questions reveal interesting differences across reasoning levels\n",
    "- The model tends to converge on 2-3 common responses even with temperature variation\n",
    "- Higher reasoning effort often produces more philosophical or abstract responses\n",
    "- Lower reasoning effort tends toward more literal or common interpretations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
